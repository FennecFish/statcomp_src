---
title: "Advanced MCMC"
author: "Naim Rashid"
date: "2/7/2019"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bftm}[1]{\ensuremath{\mathbf{#1$^{T}$}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \A \bfm{A}$'
- '$\def \b \bfm{b}$'
- '$\def \tA \bftm{A}$'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \Y \bfm{Y}$'
- '$\def \X \bfm{X}$'
- '$\def \x \bfm{x}$'
- '$\def \tx \bftm{x}$'
- '$\def \z \bfm{z}$'
- '$\def \betab \bdm{\beta}$'
- '$\def \Omegab \bdm{\Omega}$'
- '$\def \pib \bdm{\pi}$'
- '$\def \thetab \bdm{\theta}$'
- '$\def \epsilonb  \bdm{\epsilon}$'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction

In this lecture we will discuss several specific MCMC algorithms that address some of the short comings in the general methods discussed in the last lecture.  Namely, we will focus on adaptive MCMC methods, as well as reversible jump MCMC methods for bayesian variable selection.  

# Adaptive MCMC

In the previous lecture we discussed examples of MH and Gibbs Sampling where the variance of the proposal distributions was fixed across iterations.  For example, in the MH independence sampler we drew from a proposal distribution $g \sim N(0,\sigma^2)$ where $\sigma^2$ was fixed across iterations.  For the MH random walk sampler, we drew $\epsilon$ from a distribution $h$ where $h \sim U(0,1)$ or $N(0,1)$.  

As one can imagine, in certain situations the rejection rate in such samplers may be much higher or much lower than the recommended range, leading to decreased performance.  We discussed manual tuning methods in the last lecture to update the proposal distributions after several runs of MCMC and checking the output.  

An alternative, and perhaps more convenient, approach is to tune the proposal distributions while the chain is running, given the observed rejection rate.  For example, widening the proposal distributions if the rejection rate is too low, or narrowing it if the rejection rate is too high.  Such approaches are called Adaptive MCMC (AMCMC) methods.   

Implementations of AMCMC methods have been around for some time, however, as one can imagine, proving that the stationarity of such chains result from adaptive methods was difficult.  More recently we have seen some theoretical work that discusses this point, and also the conditions under which they we can expect stationary behavior of such chains.  We will see how we must craft AMCMC algorithms to satisfy such conditions.  

In general, we must take care in terms of setting up such chains, as we do not want the chain to be too heavily influenced by the tuning.  With the Markov assumption we assume that the current state is only on the previous one, however we run the risk with too much tuning that the current state may depend on the full history of the chain itself.  Another issue we need to worry about is whether the dependence on the previous iteration with tuning is too strong, resulting in the sampler not fully exploring the state space.  In general, it is best to ease the amount of tuning being performed as the chain progresses to allow it to reach its stationary distribution. 

## Conditions for AMCMC algorithms to ensure stationarity

We can assume that and AMCMC algorithm is ergodic with respect to its target distribution $f$ if the the algorithm satisfies two conditions. 

Ths first is that of **diminishing adaptations**, that is, as $t\rightarrow \infty$, the parameters in the chain will depend less and less on the earlier states of the chain. Practically speaking, this is met by ensuring that as the chain progresses, the modifications to the proposal distribution parameters occurs less and less frequently, or occurs in smaller and smaller amounts. This is an example of easing off of the tuning as the chain progresses. One can imagine that frequent tuning updates as $t \rightarrow \infty$ would heavily influence in the chain and interfere with the reaching of its stationary distribution.  

The second condition is that of **bounded convergence**, or containment.  Let $D^{(t)}$ represent the largest posssible distance between the target stationary distribution and the stationary distribution of the transition kernel of the MCMC algorithm at time $t$.  Let $M^{(t)}(\epsilon)$ be the smallest $t$ such that $D^{(t)} < \epsilon$. The bounded convergence condition states that the stochastic process $M^{(t)}(\epsilon)$ is bounded in probability for any $\epsilon > 0$.  In other words, the time it takes for the two distibutions to get sufficiently close is bounded in probability.  In practice these conditions
lead to simpler, verifiable conditions that are relatively easy to check (we will talk more about this later).

## Adaptive Random Walk Metropolis-within-Gibbs

In the last lecture we alluded to the scenario when one may not have the necessary conditional distributions in closed form to implement the Gibbs sampler, or that it may not be easy to sample from such conditional distributions.  In such cases, we can apply the Metropolis-within-Gibbs (MwG) algorithm, where we replace each Gibbs step with a Metropolis step.  

If we recall, we defined $\boldsymbol{X} = (X_1,\ldots,X_p)^T$ and $\boldsymbol{X}_{-i} = (X_1,\ldots,X_{i-1},,X_{i+1},\ldots,X_p)^T$, the elements of $\boldsymbol{X}$ minus the $i$th component. Suppose that the univariate conditional density of $X_i|\boldsymbol{X}_{-i}$, denoted as $f(X_i|\boldsymbol{X}_{-i})$, is easily sampled for $i = 1,\ldots,p$. Then, we can describe a general Gibbs sampling procedure as follows:

1.  Select starting values $\boldsymbol{x}^{(0)}$, and set $t = 0$.
2.  Generate sequentially

\begin{align}
X_1^{(t+1)} &| \cdot  ∼ f(x_1 | x_2^{(t)},\ldots,x_p^{(t)}),\\
X_2^{(t+1)} &| \cdot  ∼ f(x_2 | x_1^{(t+1)},x_3^{(t)},\ldots,x_p^{(t)}),\\
X_p^{(t+1)} &| \cdot  ∼ f(x_p | x_1^{(t+1)},\ldots,x_{p-1}^{(t+1)}),
\end{align}
where $|\cdot$ denotes conditioning on the most recent updates to $\boldsymbol{X}$. 
3. Set $t = t+1$ and go to step 2.

For MwG, we replace one or more steps in the gibbs cycle in (2) above with a Metropolis step.  We can rewrite the Gibbs sampling algorithm with MwG using the following approach,  where we assume for the sake of example that we wish to utilize a random walk sampler in each MH step:

1.  Select starting values $\boldsymbol{x}^{(0)}$, and set $t = 0$.
2.  MwG update: Obtain $X_1^{(t+1)}$ via MH random walk update
    a.  Generate proposal  $X_1^{*} =  x_1^{(t)} + \epsilon$, where $\epsilon \sim N(0\sigma^2)$
    b.  Compute the MH ratio $$R({x}^{(t)},{X}^{*}) = \frac{f(X_1^{*})}{f(x_1^{(t)})}$$
    c.  Then we sample a value for $X_1^{(t+1)}$ by choosing 
    
    \begin{align} 
{X}^{(t+1)} &= \begin{cases}
  {X}^{*} \text{ with probability min} (R({x}^{(t)},{X}^{*}),1)\\      
  {x}^{(t)} \text{ otherwise.}
\end{cases}
\end{align}
3.  Then, we perform the Gibbs updates for the remaining parameters that we have closed form univariate conditionals for, where we generate in turn

\begin{align}
X_2^{(t+1)} &| \cdot  ∼ f(x_2 | x_1^{(t+1)},x_3^{(t)}\ldots,x_p^{(t)}),\\
X_3^{(t+1)} &| \cdot  ∼ f(x_3 | x_1^{(t+1)},x_2^{(t+1)},x_4^{(t)},\ldots,x_p^{(t)}),\ldots\\
X_p^{(t+1)} &| \cdot  ∼ f(x_p | x_1^{(t+1)},\ldots,x_{p-1}^{(t+1)}).
\end{align}
Here $|\cdot$ denotes conditioning on the most recent updates to $\boldsymbol{X}$. 
3. Set $t = t+1$ and go to step 2.  

Here we assume that the parameters were arranged such that the only parameter without the closed form conditional was first in the cycle. Clearly, we can generalize this to the case where more or all of the parameters lack closed form univariate conditional distributions. 

Lets illustrate this with an example, where we extend the fitting of the poisson GLMM problem via MCEM to higher dimensional setting, where we assume our model has four fixed effects predictors, each with their own subject-level random slope, and also a random intercept. 

### Example:  Fitting a higher dimensionsional poisson GLMM via MCEM

For this example, let us assume a similar setup as the motivating problem in the last lecture, except now we assume that $\log(\lambda_{ij}) = \boldsymbol{x}_{ij}\boldsymbol{\beta} + \boldsymbol{z}_{ij}\boldsymbol{\gamma}_i$, where  $\boldsymbol{x}_{ij} = (1, x_{ij1}, x_{ij2}, x_{ij3}, x_{ij4})$ with corresponding $\boldsymbol{\beta} = (\beta_0,\beta_1,\beta_2\beta_3,\beta_4)^T$, and  $\boldsymbol{\gamma}_i = (\gamma_{i0},\gamma_{i1},\gamma_{i2},\gamma_{i3},\gamma_{i4})$.   Given that the random effects are now multivariate,we replace $\phi(\boldsymbol{\gamma}_i| 0, \sigma^2_{\gamma})$ with $\phi(\boldsymbol{\gamma}_i| 0, \Sigma_{\gamma})  \sim MVN(0,\Sigma_{\gamma})$, where $\Sigma_{\gamma})$ is a $q \times q$ covariance matrix pertaining to the random effects vector $\boldsymbol{\gamma}_i$ to be estimated. Since we are assuming that each predictor has their own random slope, and also assume a random intercept, here $\boldsymbol{z}_{ij} = \boldsymbol{x}_{ij}$.  

That is, relative to the previous problem, we are increasing the dimension of both the fixed and random effects. The new likelihood takes a similar form

$$L(\boldsymbol{\beta}, \sigma^2_{\gamma} | \boldsymbol{y}) = \prod_{i = 1}^{22}\int \left[f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i})\phi(\boldsymbol{\gamma}_i| 0, \Sigma_{\gamma}) \right] d\boldsymbol{\gamma}_i.$$

Clearly the integral here is multidimensional with respect to the random effect vector $\boldsymbol{\gamma}_i$, which has in this setting dimension $q = 5$.  In contrast, in the previous problem $q = 1$.  The Q-function pertaining to the $i$th subject will then take the following form and approximation:

\begin{aligned}
Q_i(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) &= \int \left[\log (f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) + \log\left(\phi(\boldsymbol{\gamma}_i| 0, \Sigma_{\gamma})\right)\right]f(\boldsymbol{\gamma}_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})d\boldsymbol{\gamma}_i\\
&= \int \left[\sum_{j = 1}^{5}\log(f(y_{ij} | \lambda_{ij}) + \log\left(\phi(\boldsymbol{\gamma}_i| 0, \Sigma_{\gamma})\right)\right]f(\boldsymbol{\gamma}_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})d\boldsymbol{\gamma}_i\\
&=\frac{1}{M}\sum_{k = 1}^M\left[\sum_{j = 1}^{5}\log\left(f(y_{ij} | \lambda_{ijk})\right) + \log\left(\phi(X_k| 0, \Sigma_{\gamma})\right)\right]
\end{aligned}

Lets simulate some data from such a model below.  We we will a similar structure as the previous Poisson GLMM example, assuming $n = 22$ subjects, $n_i = 5$ measurements per subjects, and a random intercept.  We will also add on 4 fixed effects predictors, each with their own random slope as well.  Giving the increase dimenson however we will bump $n$ to 50.

```{r}
library(mvtnorm)
library(lme4)

## set seed
set.seed(1)

## set variables
# number of samples
n = 100

# number of timepoints measured per sample 
ni = 5

# total number of measurements
N = n*ni

# pick fixed effects coefficient vector
beta = matrix(c(0.5, .25, .25, .25, .25), ncol = 1)

# pick random effect variances (assuming 0 covariance)
Sigma_gamma = diag(c(0.25, 1, 2, 3, 1))

# set dimensions of fixed effects and random effects
p = length(beta)
q = ncol(Sigma_gamma) 

# simulate data for p time varying covariates, measured per time point
mat = matrix(rnorm(N*(p-1), mean = 0, sd = 0.5), nrow = N, ncol = p-1) 

# Specify subject ID for each row in mat
ID = rep(1:n, each = ni)
  
# add intercept
X = cbind(rep(1, n), mat)
colnames(X) = c("int", "x1","x2","x3","x4")
  
# create y vector to hold observation
y = rep(0, N)

# now simulate yi, i = 1,..,22

for(i in 1:n){
  
  # get subject  i indices
  subjecti = which(ID == i)
  
  # get time-varying covariates for subject i, dropping ID colum 
  Xi = X[subjecti,]
  
  # here we assume random effect matrix Zi for subject i same as Xi
  Zi = Xi
  
  # draw single q dimensional random effect vector for subject i, gammai
  gammai = rmvnorm(n = 1, 
                   mean = rep(0, ncol(Sigma_gamma)),
                   sigma = Sigma_gamma
            )
  gammai = matrix(gammai, ncol =1)
  
  # generate vector of ni observations for subject i
  lambdai = exp(Xi%*%beta + Zi%*%gammai)
  y[subjecti] = rpois(ni, lambdai)
  
}

```

Now that we have simulated the data, lets check to see if we did this properly by fitting the desired model using glmer.  Note the run time

```{r}
start =  Sys.time()
fit.glmer = glmer(y ~ X - 1 + (X - 1 | ID) ,family = poisson())
end =  Sys.time()

summary(fit.glmer)

print(end - start)
```

Looks good, and run time took relatively longer than our previous example!  This is with the laplace approximation in glmer.  We can see that performance slows down even for laplace in higher dimensions.  In this case we also observed problems with the convergence of the model as well.   

Now, lets try to fit the model via MCEM using an independence sampler. Here, using the MH independence sampler implies drawing from a multivariate proposal distribution.  Lets keep track of the acceptace rate while it runs.  We will set up the function for the independence sampler with a proposal density $g \sim MVN(\boldsymbol{0}, \Sigma_\gamma^{(t)})$, where $\Sigma_\gamma^{(t)})$ is the estimate of the $q \times q$ random effects covariance matrix at step $t$ of the MCEM algorithm.  Some helper functions are below

```{r}
### function for the log likelihood for ith subject 
f = function(x,yi, Xi, betat, Sigma_gammat){

  # here we assume random effect matrix Zi for subject i same as Xi
  Zi = Xi
  
  # calculate lambdai
  lambdai = exp(Xi%*%betat + Zi%*%matrix(x, ncol = 1))
  
  # sum across repeated observations for poisson portion
  lli = sum(dpois(yi, lambdai, log = T)) 
  
  # dont forget to include the MVN log likelihood
  lli = lli  + dmvnorm(x,sigma = Sigma_gammat, log = T)
  
  return(lli)
}

### log proposal density function
g = function(x, Sigma_gammat){
  dmvnorm(x, sigma = Sigma_gammat, log = T)
}

### proposal function, MVN(0, Sigma)
g.sim = function(Sigma_gammat){
  rmvnorm(1, sigma = Sigma_gammat)
}

### calculate MH ratio given f and g, x is the proposal, xt is the current value from the chain
R = function(xt,x, f, g, yi, Xi, betat, Sigma_gammat){
  # log numerator - log denominator
  logR = ( f(x, yi, Xi, betat, Sigma_gammat) + g(xt, Sigma_gammat) ) - ( f(xt,yi, Xi, betat, Sigma_gammat) + g(x , Sigma_gammat) )
  R = exp(logR)
  return(R)
}
```

Now that thats done, let’s write the function for the MH algorithm here 

```{r}

mh.independence.sampler = function(yi, Xi, betat, Sigma_gammat, M, prev.gamma.i = NULL){

  # get dimension of gammai
  q = ncol(Sigma_gammat)
  
  # initialize the chain vector
  x.indep.chain = matrix(0, M, q)
  
  if(is.null(prev.gamma.i)){
    # Simulate initial draw from proposal density g
    x.indep.chain[1,] = g.sim(Sigma_gammat)
  }else{
    # if last value from previous chain avail, start there
    x.indep.chain[1,] = prev.gamma.i    
  }
  
  
  # now start chain
  accept = 0
  for(i in 1:(M-1)){
    
    # set the value at current iteration of the chain to variable xt
    xt = x.indep.chain[i,]
    
    # draw a proposal from the proposal density
    x = g.sim(Sigma_gammat)
    
    # calculate MH ratio 
    r = min(
            R(xt, x, f, g, yi, Xi, betat, Sigma_gammat),
            1
          )
    
    # Generate draw from bernoulli(p).
    # Alternatively, can directly compare ratio to 
    # a U(0,1) draw as we did with Rejection Sampling
    keep = rbinom(1, 1, r)
    
    if(keep == 1){
      # if keep = 1, then set next iteration equal to then proposal
      x.indep.chain[i+1,] = x
      #  update number of acceptacnes
      accept = accept + 1
    }else{
      # otherwise, carry over value from the current iteration
      x.indep.chain[i+1,] = xt
    }
  }
  
  return(list(gammai = x.indep.chain, ar = accept/M))
}
```

To simplify things, lets just roll this into an E-step function to calculate the Q-function

```{r}
e.step= function(y, X, ID, betat, Sigma_gammat, M, n, ni, sampler, burn.in = 200,prev.gamma = NULL){
  
  # initialize Q-function
  Qfunction = 0
  
  # matrix to hold chains from each subject
  gamma = matrix(0, n*M, ncol(Sigma_gammat))
  
  # vector to hold subject acceptance rates
  ar = matrix(0, n, q)
  
  # vector to hold offset values Zi %*% gammai, yaug, Xaug in M step
  N = ni*n
  offset = yaug = rep(0, N*M)
  Xaug = matrix(0, nrow = N*M, ncol = ncol(X))
  
  # loop over observations
  for(i in 1:n){
      
      # subject i indices
      subjecti = which(ID == i)
      
      # grab subject i data
      yi = y[subjecti]
      Xi = X[subjecti,]
      
      # create chain of length M per observation 
      if(is.null(prev.gamma)){
        # if no previous chain available
        # start from scratch and remove burn.in
        chain = sampler(
                    yi = yi, 
                    Xi = Xi,
                    betat = betat, 
                    Sigma_gammat = Sigma_gammat,
                    M = M + burn.in)
        gammai = chain$gammai[-c(1:burn.in),]
      }else{
        # if chain available from previous EM
        # restart this chain from last draw in previous chain
        chain = sampler(
                      yi = yi, 
                      Xi = Xi,
                      betat = betat, 
                      Sigma_gammat = Sigma_gammat,
                      M = M, # no burn in
                      prev.gamma = prev.gamma[i,]
                ) 
        gammai = chain$gammai
      }

      ar[i,] = chain$ar
      
      # create augmented versions for Q function calculation
      # total length is (n*ni*M) rows
      aug = rep(1:ni, M)
      yi_aug = yi[aug]
      Xi_aug = Xi[aug,]
      Zi_aug = Xi_aug
      
      # create augmented version of gammai to aid vectorization
      # repeated ni times per replicated subject 
      # total length is (n*ni*M) rows
      augg = rep(1:M, each = ni)
      gammai_aug = gammai[augg,]
      
      # calculate Q function for subject i:  poisson portion (n*ni*M)
      XBaug = Xi_aug%*%betat
      Zgammaaug = rowSums(Zi_aug * gammai_aug)
      lambdai_aug = exp(XBaug + Zgammaaug)
      Qi = sum(dpois(yi_aug, lambda = lambdai_aug, log = T))
      
      # calculate Q function for subject i:  MVN portion (n*M)
      Qi = Qi + sum(dmvnorm(gammai_aug, sigma = Sigma_gammat, log = T))
      
      # divide by M
      Qi = Qi/M
      
      # add to overall Q estimate
      Qfunction = Qfunction + Qi
      
      # save offset, yaug, xaug for later
      a = (i-1)*M*ni + 1
      b = i*M*ni
      offset[a:b] = Zgammaaug
      yaug[a:b] = yi_aug
      Xaug[a:b,] = Xi_aug
      
      # save gammai for later
      a = (i-1)*M + 1
      b = i*M
      gamma[a:b,] = gammai
    }
      
    return(list(Qfunction = Qfunction, gamma = gamma, ar = ar, offset = offset, yaug = yaug, Xaug = Xaug))
}
```


Now lets start the EM algorithm


```{r}
## set initial parameters
  tol = 10^-5
  maxit = 20
  iter = 0
  eps = Inf
  qfunction = -10000 # using Qfunction for convergence
  prev.gamma = NULL
  
## starting values
  beta = as.vector(glm(y ~ X-1, family = poisson())$coef)
  Sigma_gamma =  diag(rep(1, 5))

## fix chain length at 1000 in E-step
M = 1000

start = Sys.time()
while(eps > tol & iter < maxit){
  
  ## save old qfunction
  qfunction0 = qfunction
  
  ## obtain last chain value (Mth value) for each obs if iter > 0
  if(iter > 0){
    prev.gamma = gamma[seq(M,nrow(gamma), by = M),]
  }
  
  ## E-step
  estep = e.step(y = y, X = X, ID = ID, betat = beta, Sigma_gammat = Sigma_gamma, M = M, n = n, ni = ni, sampler = mh.independence.sampler, prev.gamma = prev.gamma)
  gamma = estep$gamma
  qfunction = estep$Qfunction
  offset = estep$offset
  yaug = estep$yaug
  Xaug = estep$Xaug
  
  ## Calculate relative change in qfunction from prior iteration
  eps  = abs(qfunction-qfunction0)/abs(qfunction0)
    
  ## Start M-step
    
  # s2gamma, MLE for sigma^2 from normal with mean 0, averaged over M
  # closed form derived from Q function approximation
  Sigma_gamma = t(gamma) %*% gamma/(n*M)

  aug = rep(1:n,each = M)
  fit = glm(yaug ~ Xaug -1, 
              family = poisson(), 
              weights = rep(1/M, nrow(Xaug)), 
              offset = offset,
              # use starting value from previous step
              start = beta
  )
  beta = as.vector(fit$coefficients)
    
  ## update iterator
  iter = iter + 1
  if(iter == maxit) warning("Iteration limit reached without convergence")
    
  ## print out info to keep track
  cat(sprintf("Iter: %d Qf: %.3f sigma_gamma1: %f beta0: %.3f beta1:%.3f eps:%f\n",iter, qfunction,diag(Sigma_gamma)[1], beta[1],beta[2], eps))
}
  end = Sys.time()
  print(end - start)
  
```

For the sake of illustration/speed we fixed $M$ to be relatively small and terminated the algorithm after 50 iterations.  In reality we may need a much larger M and more EM iterations to reach convergence. 

Lets take a look at the final acceptance rates for each subject's chains

```{r}
par(mfrow = c(2,2))
hist(estep$ar)
plot(gamma[1:M,1], type = 'l')
acf(gamma[1:M,1])
```

This does not look that great, many of the acceptance rates pertaining to each observation's chain are below 0.1.  Ideally, this could be closer to 0.23 in the multivariate setting.  

Lets try rerunning this with MwG and compare the performance.  All we will be doing here is here swapping out the sample and leaving everything else the same.  The code for the MwG sampler is below.  We update $g$ to reflect the random walk sampler, and recall that $g(\boldsymbol{x}^*|\boldsymbol{x}^{(t)}) = h(\boldsymbol{x}^*-\boldsymbol{x}^{(t)})$.  If $h$ is symmetric, which is clearly is for say a N(0,1) density, then terms related to $g$ will cancel out in the MH ratio due to symmetry of the proposal distribution, as $h(\boldsymbol{x}^*-\boldsymbol{x}^{(t)}) = h(\boldsymbol{x}^{(t)}-\boldsymbol{x}^*)$. 

```{r}
# rw density
h.sim = function(){
  rnorm(1)
}

# new MH ratio function (proposal density cancels out due to symmetry)
R = function(xt,x, f, yi, Xi, betat, Sigma_gammat){
  # log numerator - log denominator
  logR = f(x, yi, Xi, betat, Sigma_gammat) - f(xt,yi, Xi, betat, Sigma_gammat)
  R = exp(logR)
  return(R)
}


mwg.rw.sampler = function(yi, Xi, betat, Sigma_gammat, M, prev.gamma.i = NULL){

  # get dimension of gammai
  q = ncol(Sigma_gammat)
  
  # initialize the chain vector
  x.indep.chain = matrix(0, M, q)
  
  if(is.null(prev.gamma.i)){
    # Simulate initial draw from original proposal density g
    x.indep.chain[1,] = g.sim(Sigma_gammat)
  }else{
    # if last value from previous chain avail, start there
    x.indep.chain[1,] = prev.gamma.i    
  }
  
  # now start chain
  accept = rep(0,q)
  for(i in 1:(M-1)){
    
    # set the value at current iteration of the chain to variable xt
    xt = x.indep.chain[i,]
      
    #looping over chains drawing univariate proposal
    # conditions unknown here so performing MH at each gibbs step
    for(j in 1:q){  
      
      # set propsal equal to previous
      x = xt 
      
      # only update the jth component
      x[j] = x[j] + h.sim()
      
      # calculate MH ratio
      r = min(
              R(xt, x, f, yi, Xi, betat, Sigma_gammat),
              1
            )
      
      # Generate draw from bernoulli(p).
      # Alternatively, can directly compare ratio to 
      # a U(0,1) draw as we did with Rejection Sampling
      keep = rbinom(1, 1, r)
      
      if(keep == 1){
        # if keep = 1, then set next iteration equal to then proposal
        x.indep.chain[i+1,] = x
        
        # reset xt
        xt = x
        
        #  update number of acceptacnes
        accept[j] = accept[j] + 1
      }else{
        # otherwise, carry over value from the current iteration
        x.indep.chain[i+1,] = xt
      }
    }
    # end of a single gibbs cycle
  }
  # end chain 
  
  return(list(gammai = x.indep.chain, ar = accept/M))
}
```

Now lets see how this impacts performance.  We simply swap out the sampler that we use in the E step function and rerun the same code.  

```{r}
## set initial parameters
  tol = 10^-5
  iter = 0
  eps = Inf
  qfunction = -10000 # using Qfunction for convergence
  prev.gamma = NULL
  
## starting values
  beta = as.vector(glm(y ~ X-1, family = poisson())$coef)
  Sigma_gamma =  diag(rep(1, 5))

start = Sys.time()
while(eps > tol & iter < maxit){
  
  ## save old qfunction
  qfunction0 = qfunction
  
  ## obtain last chain value for each obs if iter > 0
  if(iter > 0){
    prev.gamma = gamma[seq(M,nrow(gamma), by = M),]
  }
  
  ## E-step
  estep = e.step(y = y, X = X, ID = ID, betat = beta, Sigma_gammat = Sigma_gamma, M = M, n = n, ni = ni, sampler = mwg.rw.sampler, prev.gamma = prev.gamma)
  gamma = estep$gamma
  qfunction = estep$Qfunction
  offset = estep$offset
  yaug = estep$yaug
  Xaug = estep$Xaug
  
  ## Calculate relative change in qfunction from prior iteration
  eps  = abs(qfunction-qfunction0)/abs(qfunction0)
    
  ## Start M-step
    
  # s2gamma, MLE for sigma^2 from normal with mean 0, averaged over M
  # closed form derived from Q function approximation
  Sigma_gamma = t(gamma) %*% gamma/(n*M)

  aug = rep(1:n,each = M)
  fit = glm(yaug ~ Xaug -1, 
              family = poisson(), 
              weights = rep(1/M, nrow(Xaug)), 
              offset = offset,
              # use starting value from previous step
              start = beta
  )
  beta = as.vector(fit$coefficients)
    
    ## update iterator
    iter = iter + 1
    if(iter == maxit) warning("Iteration limit reached without convergence")
    
    ## print out info to keep track
    cat(sprintf("Iter: %d Qf: %.3f sigma_gamma1: %f beta0: %.3f beta1:%.3f eps:%f\n",iter, qfunction,diag(Sigma_gamma)[1], beta[1],beta[2], eps))
  }
  end = Sys.time()
  print(end - start)
  
```

Again for the sake of illustration/speed, we did not increase M to facilitation convergence, a much larger value is needed to do so.  Now lets compare these results with glmer

```{r}
# glmer
summary(fit.glmer)

```

Compare these to ours:

```{r}
# MCEM fixed effects
print(beta)

# MCEM RE variance estimates (diagonal of covariate)
print(diag(Sigma_gamma))

```

Now lets take a look at the acceptance rates

```{r}
head(estep$ar)

par(mfrow = c(2,2))
hist(estep$ar[,1], xlim = c(0,1));abline(v = 0.44);
hist(estep$ar[,2], xlim = c(0,1));abline(v = 0.44);
hist(estep$ar[,3], xlim = c(0,1));abline(v = 0.44);
hist(estep$ar[,4], xlim = c(0,1));abline(v = 0.44);

```

The acceptance rates are better than the first time around, and we see that they are in some cases close to the optimal value of 0.44 for univariate proposals.  However, this is not the case across all samples, and across various components of $\boldsymbol{\gamma}$.  We can however improve on this using adaptive methods, discussed in the next section. 

Before doing so lets look at some diagnostics

```{r}
par(mfrow = c(1,2))
plot(gamma[1:M,1], type = 'l')
acf(gamma[1:M,1])
```

## Adaptive Metropolis-within-Gibbs

When we monitor the acceptance rate of the MH normal RW sampler in each gibbs step, we can see that the acceptance rate is not in the optimal range in some cadses.  To address this issue, we can tune the variance of the propsal distribution within each Gibbs MH step so that we can get the acceptance rates closer to the region that we desire.  To do this, we will add what is called an adaptation step to the MwG algorithm described above.  Let use assume that we perform this adaption step only at specific times $t \in \{50, 100, 150, \ldots\}$, where we denote these times as "batch times" $T_b$, where $b = 0,1,\ldots$.  Here $T_1 = 50$ for example.  At each batch time we tune the variance of the proposal given the observed acceptance rate for iterations just in that batch.  We can increase the spacing between batch times given the mixing performance of the chain and the specific problem at hand, but 50 is a typical number.  

Then, we can perform AMwG using the following approach:

1.  Select starting values $\boldsymbol{x}^{(0)}$, and set $t = 0$.  Set a batching schedule $\{T_b\}$ for $b = 0, 1, \ldots$ and set $b = 0$.  Lets initialize $\sigma^2_{0} = 1$. 
2.  MwG update: Obtain $X_1^{(t+1)}$ via MH random walk update
    a.  Generate proposal  $X_1^{*} =  x_1^{(t)} + \epsilon$, where $\epsilon \sim N(0,\sigma_b^2)$
    b.  Compute the MH ratio $$R({x}^{(t)},{X}^{*}) = \frac{f(X_1^{*})}{f(x_1^{(t)})}$$
    c.  Then we sample a value for $X_1^{(t+1)}$ by choosing 
    
    \begin{align} 
{X}^{(t+1)} &= \begin{cases}
  {X}^{*} \text{ with probability min} (R({x}^{(t)},{X}^{*}),1)\\      
  {x}^{(t)} \text{ otherwise.}
\end{cases}
\end{align}
3.  Then we perform the Gibbs updates for the remaining parameters that we have closed form univariate conditionals for, where we generate in turn

\begin{align}
X_2^{(t+1)} &| \cdot  ∼ f(x_2 | x_1^{(t+1)},x_3^{(t)}\ldots,x_p^{(t)}),\\
X_3^{(t+1)} &| \cdot  ∼ f(x_3 | x_1^{(t+1)},x_2^{(t+1)},x_4^{(t)},\ldots,x_p^{(t)}),\\
X_p^{(t+1)} &| \cdot  ∼ f(x_p | x_1^{(t+1)},\ldots,x_{p-1}^{(t+1)}),
\end{align}
4.  Adaptation Step: when $t = T_{b+1}$,
    a.  Update the variance of the proposal distribution $$\log(\sigma_{b+1}) = \log(\sigma_{b}) \pm \delta_{(b+1)},$$ where $\delta_{(b+1)}$ is called the adaptation factor.  We **add** $\delta_{(b+1)}$  when the acceptance rate in step 2(c) above is smaller than say 0.44 (ideal rate for univariate normally distributed target and proposal distributions) and **subtract** when the rate is larger than 0.44. In this manner we are able to automatically tune the variance of the proposal to keep the acceptance rate in the range that we woud like it.  A common choice is $\delta_{(b+1)} = \min(0.01, 1/\sqrt{T_b})$, where 0.01 is an arbitrary choice that limits the amount of adaptation for smaller values of $T_b$.
    b.  Set $b  = b +1$
5.  Increment $t$ and return to step 2.
  
As discussed in the last lecture, we still need to check convergence criteria pertaining to the two conditions we laid out earlier.  For MwG, this simplies to checking whether $\delta_{(b+1) \rightarrow 0$ as $b \rightarrow \infty$ to satisfy the  diminishing adaptation condition.  The bounded convergence condition is met if $\log(\sigma_b) \in [−Q,Q]$ where $Q<\infty$.  As we can see, these conditions are relatively easy to check.  The approach discussed above can extend to almost any random walk distribution $h$, with the caveat that the MH ratio would have to be updated to include the proposal distributions if $h$ is not symmetric. 

In the example we gave above, we had only one parameter where the conditional was not known in closed form.  As one can imagine, the AMwG algorithm will be particular useful in cases where the dimension of the parameters is large.  In such cases where multiple parmeters are have unknown conditions, it may be helpful to have separate adaptation steps for each parameter, each with their own adaptive proposal distribution variances.  In this manner, we can tune the propsal in each MwG step separately from the others.  As one can imagine, some parameters may have very different acceptance rates as others. 

Let us now apply this to the problem at hand

```{r}
# rw density
h.sim = function(var = 1){
  rnorm(1, mean = 0, sd = sqrt(var))
}

adaptive.mwg.rw.sampler = function(yi, Xi, betat, Sigma_gammat, M, prev.gamma.i = NULL, b = 50){

  # get dimension of gammai
  q = ncol(Sigma_gammat)
  
  # initialize the chain vector
  x.indep.chain = matrix(0, M, q)
  
      
  # initialize proposal variance
  prop.var = rep(1, q)
  
  if(is.null(prev.gamma.i)){
    # Simulate initial draw from original proposal density g
    x.indep.chain[1,] = g.sim(Sigma_gammat)
  }else{
    # if last value from previous chain avail, start there
    x.indep.chain[1,] = prev.gamma.i  
    
  }
  
  #intialize batch index
  batch = 0
  
  # now start chain
  accept = b.accept = rep(0,q)
  
  
  for(i in 1:(M-1)){
    
    # set the value at current iteration of the chain to variable xt
    xt = x.indep.chain[i,]
      
    #looping over chains drawing univariate proposal
    # conditions unknown here so performing MH at each gibbs step
    for(j in 1:q){  
      
      # set propsal equal to previous
      x = xt 
      
      # only update the jth component
      # proposal variance is based on vector
      x[j] = x[j] + h.sim(var = prop.var[j])
      
      # calculate MH ratio
      r = min(
              R(xt, x, f, yi, Xi, betat, Sigma_gammat),
              1
            )
      
      # Generate draw from bernoulli(p).
      # Alternatively, can directly compare ratio to 
      # a U(0,1) draw as we did with Rejection Sampling
      keep = rbinom(1, 1, r)
      
      if(keep == 1){
        # if keep = 1, then set next iteration equal to then proposal
        x.indep.chain[i+1,] = x
        
        # reset xt
        xt = x
        
        #  update number of acceptances
        accept[j] = accept[j] + 1
        b.accept[j] = b.accept[j] + 1 
      }else{
        # otherwise, carry over value from the current iteration
        x.indep.chain[i+1,] = xt
      }
    }
    # end of a single gibbs cycle
    
    # if at end of batch
    if(floor(i/b) == ceiling(i/b)){
      
      # increment for proposal variance
      delta.b = min(0.01, 1/sqrt(i))
      
      # loop over proposal density variance vector
      for(j in 1:q){
        if(b.accept[j]/b < 0.44){
          # if less, add to variance
          prop.var[j] = log(sqrt(prop.var[j])) + delta.b        
        }else{
          # otherwise, subtract
          prop.var[j] = log(sqrt(prop.var[j])) - delta.b
        }
      }
      
      # tranform back from log sqrt scale
      prop.var = exp(prop.var)^2
      # reset batch counter
      b.accept = rep(0, q)
      
      # increment batch index
      batch = batch + 1
    } 
    
  }
  # end chain 
  
  return(list(gammai = x.indep.chain, ar = accept/M, prop.var = prop.var))
}
```


Now, lets swap out the sampler and rerun

```{r}
## set initial parameters
  tol = 10^-5
  iter = 0
  eps = Inf
  qfunction = -10000 # using Qfunction for convergence
  prev.gamma = NULL
  
## starting values
  beta = as.vector(glm(y ~ X-1, family = poisson())$coef)
  Sigma_gamma =  diag(rep(1, 5))


start = Sys.time()
while(eps > tol & iter < maxit){
  
  ## save old qfunction
  qfunction0 = qfunction
  
  ## obtain last chain value for each obs if iter > 0
  if(iter > 0){
    prev.gamma = gamma[seq(M,nrow(gamma), by = M),]
  }
  
  ## E-step
  estep = e.step(y = y, X = X, ID = ID, betat = beta, Sigma_gammat = Sigma_gamma, M = M, n = n, ni = ni, sampler = adaptive.mwg.rw.sampler, prev.gamma = prev.gamma)
  gamma = estep$gamma
  qfunction = estep$Qfunction
  offset = estep$offset
  yaug = estep$yaug
  Xaug = estep$Xaug
  
  ## Calculate relative change in qfunction from prior iteration
  eps  = abs(qfunction-qfunction0)/abs(qfunction0)
    
  ## Start M-step
    
  # s2gamma, MLE for sigma^2 from normal with mean 0, averaged over M
  # closed form derived from Q function approximation
  Sigma_gamma = t(gamma) %*% gamma/(n*M)

  aug = rep(1:n,each = M)
  fit = glm(yaug ~ Xaug -1, 
              family = poisson(), 
              weights = rep(1/M, nrow(Xaug)), 
              offset = offset,
              # use starting value from previous step
              start = beta
  )
  beta = as.vector(fit$coefficients)
    
    ## update iterator
    iter = iter + 1
    if(iter == maxit) warning("Iteration limit reached without convergence")
    
    ## print out info to keep track
    cat(sprintf("Iter: %d Qf: %.3f sigma_gamma1: %f beta0: %.3f beta1:%.3f eps:%f\n",iter, qfunction,diag(Sigma_gamma)[1], beta[1],beta[2], eps))
  }
  end = Sys.time()
  print(end - start)
  
```

Now lets take a look at the acceptance rates

```{r}
head(estep$ar)

par(mfrow = c(2,2))
hist(estep$ar[,1], xlim = c(0,1));abline(v = 0.44);
hist(estep$ar[,2], xlim = c(0,1));abline(v = 0.44);
hist(estep$ar[,3], xlim = c(0,1));abline(v = 0.44);
hist(estep$ar[,4], xlim = c(0,1));abline(v = 0.44);

```

The acceptance rates are better than the first time around, and we see that they are in some cases close to the optimal value of 0.44 for univariate proposals.  However, this is not the case across all samples, and across various components of $\boldsymbol{\gamma}$.  We can however improve on this using adaptive methods, discussed in the next section. 

Before doing so lets look at some traceplots

```{r}
par(mfrow = c(1,2))
plot(gamma[1:M,1], type = 'l')
acf(gamma[1:M,1])
```

## Adaptive Metropolis Algorithm

In our previous discussion of the Metropolis algorithm, we mentioned that a good proposal distribution should draw samples that cover the support of the stationary distribution in a reasonable number of iterations, and also produces candidate values that are not accepted or rejected too frequently. 

The Adaptive Metropolis (AM) algorithm is a variant of the Metropolis algorithm where one instead updates the variance of the proposal distribution as the algorithm runs.  Similar to the previous section, the goal of this update is to ensure that the acceptance rate of the sample is in a suitable range. We can think of AM as a one-step RW Metropolis algorithm with a normal proposal distribution, where the variance of tihs proposal distribution is updated using the previous iterations of the chain.

Lets consider a RW MH sampler where the proposal density is $N(\boldsymbol{X}^{(t)}, \Sigma$. Clearly we assume here that $\boldsymbol{X}^{(t)}$ is multivariate, and therefore we have a multivariate proposal distribution.  For the AM sampler, at each iteration of the chain, a candidate value $\boldsymbol{X}^∗$ is sampled from a proposal distribution $N(\boldsymbol{X}^{(t)}, \lambda\Sigma^{(t)})$, where the goal is to tune the covariance matrix $\Sigma^{(t)}$ to improve the acceptance rate. 

For a $d$-dimensional spherical multivariate normal target distribution where $\Sigma_\pi$ is the true covariance matrix of the target distribution, the proposal distribution $(2.382/p)\Sigma_\pi$ has been shown to be optimal with a and optiomal acceptance rate of 44% when $p = 1$.  This optimal acceptance rate decreases to 23% as $p$ increases. 

Thus, in one version of the AW algorithm, $\lambda$ is set to $(2.382/p)$. Since 	$\Sigma_\pi$ is unknown, it is estimated based on previous iterations of the chain. An adaptation parameter $\gamma^{(t)}$ is used to blend	$\Sigma^{(t)}$ and $\Sigma^{(t+1)}$ in such a way that the diminishing adaptation condition will be upheld. A parameter $\mu^{(t)}$ is also introduced and estimated adaptively. This is used to estimate the covariance matrix $\Sigma^{(t+1)}$.

This adaptive Metropolis algorithm begins at $t = 0$ with the selection of $\boldsymbol{x}^{(0)} = \boldsymbol{X}^{(0)}$ drawn at random from some starting distribution $g$. Similarly, initialize $\mu^{(0)}$ and $\Sigma^{(0)}$.  Common choices are $\mu^{(0)} = 0$ and 	$\Sigma^{(0)} = \boldsymbol{I}$. Given $\boldsymbol{X}^{(t)} = \boldsymbol{x}^{(t)}$, $\mu^{(t)}$, and $\Sigma^{(t)}$, the
algorithm generates $\boldsymbol{X}^{(t+1)}$ as follows:

1. Sample a candidate value $\boldsymbol{X}^{*}$ from the proposal distribution $N(\boldsymbol{X}^{(t)}, \lambda\Sigma^{(t)})$, where $\lambda = (2.382/p)$ .
2. Then we sample a value for $\boldsymbol{X}^{(t+1)}$ by choosing 
    
\begin{align} 
{X}^{(t+1)} &= \begin{cases}
  {X}^{*} \text{ with probability min} (R(\boldsymbol{x}^{(t)},\boldsymbol{X}^{*}),1)\\      
  {x}^{(t)} \text{ otherwise.}
\end{cases}
\end{align}
3. Adaptation step: Update the proposal distribution variance in two steps:
\begin{align}
\mu^{(t+1)} &= \mu^{(t)} + \gamma^{(t+1)}(\boldsymbol{X}^{(t+1)} -\mu^{(t)} ),\\
\Sigma^{(t+1)} &= \Sigma^{(t)} + \gamma^{(t+1)}\left[(\boldsymbol{X}^{(t+1)} -\mu^{(t)} )(\boldsymbol{X}^{(t+1)} -\mu^{(t)} )^T - \Sigma^{(t)}\right].\\
\end{align}

Here $\gamma^{(t+1)}$ is an adaptation parameter with values chosen by the user. For
example, $\gamma^{(t+1)} = 1/(t + 1)$ is a reasonable choice.

4. Increment $t$ and return to step 1.

The updating formula for 	$\Sigma^{(t)}$ is constructed so that it is computationally quick
to calculate and so that the adaptation diminishes as the number of iterations increases.
To uphold the diminishing adaptation condition, it is required that $lim_{t\rightarrow \infty} \gamma^{(t)}  = 0$. The additional condition that $\sum_{t = 0}^\infty\gamma^{(t)} = \infty $ allows the sequenc e $\Sigma^{(t)}$ to move an infinite distance from its initial value. It may make sense to adapt $\lambda$ as well in step 3, such that $$\log(\lambda^{(t+1)}) = \log(\lambda^{(t)}) + \gamma^{(t+1)}(R(\boldsymbol{x}^{(t)},\boldsymbol{X}^{*}) - a) $$ where $a$ denotes the target acceptance rate (e.g., 0.234 for higher-dimensional
problems).

One drawback with the AM algorithm above is that all components within the entire vector of parameters must be accepted or rejected simultaneously. Clearly this is not efficient when the problem is high dimensional. An alternative is to develop a componentwise hybrid Gibbs AM algorithm where each component is given its own scaling parameter $\lambda^{(t)}$ and is accepted/rejected separately in step 2. In this case, the constant $a$ is usually set to a higher value, for example, $a = 0.44$, since now components
are updated individually. Another variation is to carry out the adaptation in batches instead of
every iteration. With the batching strategy, the adaptation (step 3 in the adaptive
Metropolis algorithm) is only implemented at predetermined times.







