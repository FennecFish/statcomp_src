---
title: "Sparse data manipulation"
author: "Michael Love"
date: 11/7/2018
output: html_document
---

In this last lecture note on large data manipulation in R, we change
tactics a bit. Previously we discussed fast reading and subsetting
with *data.table*, and the advantages of *SQLite* vs *HDF5* for
storing collections of tables in a single file, and then working with
these in R using the *RSQLite* and *rhdf5* libraries. Here we discuss
an alternative approach for dealing with large arrays in which many
of the features are equal to zero. There are special classes and
methods in R that allow us to work with such data in a memory and
computationally efficient manner. These data are typically referred to
as *sparse* data, in that the non-zero elements of the array are
sparse. We will focus in this lecture note on the classes in the
*Matrix* package, and some functionality in the *glmnet* package for
fitting regularized linear or generalized linear models to sparse
feature matrices.

Let's dive right into representing sparse matrices. Here we have a
large-ish matrix wherein the non-zero elements make up only ~5% of the
total:

```{r}
m <- matrix(rbinom(1e6, 1, .05), ncol=1e3)
m[1:5,1:5]
sum(m)
prod(dim(m))
```

This matrix takes up about 4 Mb in memory:

```{r}
print(object.size(m), units="Mb")
```

That's actually not so big that we encounter problems on a laptop
computer, but if we multiply either or both of the dimensions by a
factor of 1000, we will start to hit a limit in terms of working with
the matrix.

Let's get a sense of how much space we save if we represent this as a
sparse matrix.

```{r}
library(Matrix)
mm <- as(m, "sparseMatrix")
mm[1:5,1:5]
sum(mm)
print(object.size(mm), units="Mb")
as.numeric(object.size(m)/object.size(mm))
```

The sparse version takes up less than 1/6 of the space of the *dense*
version. Note that this construction doesn't make any sense: we would
never want to first build the memory-intensive dense version of the matrix
and then convert down to the sparse version. Instead, we would use the
`sparseMatrix` function to build the matrix by specifying only the
non-zero elements, and where they occur.

First look up the help page for `sparseMatrix`:

```{r eval=FALSE}
?sparseMatrix
```

The most common way to construct a sparse matrix would be to specify
`i`, `j`, and `x` (this last argument optional, if not included, the
values will be equal to 1).

```{r}
s <- sparseMatrix(i=c(1,3,5),j=c(1,2,3),x=c(4,5,6),dims=list(6,4))
s
```

This creates an object of type `dgCMatrix`. Take a look at the help
page for this class

```{r eval=FALSE}
?dgCMatrix-class
```

You can see that this class is *column-oriented* which means it should 
be faster to index columns of these objects than rows. Likewise,
if we had not specified `x`, it would also be column-oriented by
default, but instead it would be `ngCMatrix`. Let's do a
microbenchmark to see the degree of difference. For this example,
column indexing is about twice as fast.

```{r}
library(microbenchmark)
n <- 1e6
nn <- 1e3
s <- sparseMatrix(i=sample(n,nn),
                  j=sample(n,nn),
                  dims=list(n,n))
microbenchmark(sum(s[,10]),sum(s[10,]))
```
