---
title: "Support vector machines"
author: "Michael Love"
date: 12/15/2018
output: html_document
---

Support vector machines (SVM) are a very popular machine learning
method for binary classification, which can be solved efficiently even
for large datasets. SVM have a number of desirable features,
including: ability to perform classification in a non-linear space
using kernels, tend to have good generalization to new data, and work
well in very high dimensional space.

# First look at SVM

Here we will begin by demonstrating how SVM differ from other linear
classifiers, such as LDA (linear discriminant analysis) on simulated
data (two dimensional, so we can easily visualize the hyperplane that
defines the classification boundary). We will then show the objective
function that the SVM minimizes (in the completely separable case),
show how this can be solved with quadratic programming, and then
demonstrate the use of SVM on a real dataset.

First we construct some data that happens to be completely separable
by a hyperplane in two dimensions:

```{r include=FALSE}
knitr::opts_chunk$set(fig.width=6, cache=TRUE)
```

```{r}
set.seed(2)
n <- 100
x1 <- c(rnorm(n),rnorm(n,5))
x2 <- c(rnorm(n),rnorm(n,5))
x1 <- scale(x1)
x2 <- scale(x2)
y <- factor(rep(c(-1,1),each=n))
dat <- data.frame(y,x1,x2)
library(ggplot2)
ggplot(dat, aes(x1,x2,col=y)) + geom_point()
```

Using LDA gives us the following classification boundary. In this
case, the boundary line is given by `-coefs[1]/coefs[2]`, but instead
we use code which generalizes for other cases (including multiple
classes, or non-linear boundaries as we will explore below). As you
can recall, the boundary line for the LDA is determined by the
likelihood ratio where two Gaussian distributions are fit to the data
of the two classes. We draw the center of the Gaussians with a cross:

```{r}
library(caret)
x <- data.frame(x1,x2)
lfit <- train(x, y, method="lda")
coefs <- lfit$finalModel$scaling
means <- data.frame(lfit$finalModel$means, y=factor(c(-1,1)))
s <- seq(from=-2,to=2,length=400)
grid <- expand.grid(x1=s,x2=s)
grid$y <- as.numeric(predict(lfit, newdata=grid))
ggplot(dat, aes(x1,x2,col=y)) + geom_point() +
  geom_point(data=means,shape=10,size=10,stroke=2,alpha=.5,show.legend=FALSE) +
  geom_contour(data=grid, aes(x1,x2,z=y), breaks=1.5, col="black")
```

We will go ahead a fit a *linear-kernel* SVM (to be explained later)
to the same data. We will see a boundary as above, but the slope is a
bit different. 

```{r}
fit <- train(x, y, method="svmLinear", tuneGrid=data.frame(C=100))
alpha <- fit$finalModel@alpha[[1]]
sv <- as.data.frame(x[fit$finalModel@SVindex,]) # the "support vectors"
sv.y <- 2 * (as.numeric(y[fit$finalModel@SVindex]) - 1.5)
w <- colSums(alpha * sv.y * as.matrix(sv))
b <- fit$finalModel@b
grid <- expand.grid(x1=s,x2=s)
grid$y.cont <- (as.matrix(grid[,1:2]) %*% w - b)[,1]
ggplot(dat, aes(x1,x2,col=y)) + geom_point() + 
  geom_point(data=sv, col="black", size=5, shape=21) +
  geom_contour(data=grid, aes(x1,x2,z=y.cont), breaks=c(-1,0,1), col="black")
```

The key difference is that, instead of modeling the data as two
Gaussians, the SVM has attempted to put the widest margin between the
two groups of samples. This ends up being equivalent to finding a set
of points which define the boundary between the two groups, and
putting a wide band between those sets of points. The code may not
make much sense now, but it is extracting the key parameters *w* and
*b* which define the following rules:

$$ w^T x - b = 1 $$

$$ w^T x - b = -1 $$

Anything on or above the line defined by the first equation will be 
classified as +1, while anything on or below the line in the second
equation will be classified as -1. We then draw the lines for 1, 0,
and -1 to show the boundaries and center of the margin dividing the
two groups. The lines pass through a set of data points, these are
called the *support vectors*. It is the nature of the constrained
optimization of the SVM that a subset (sometimes small) of the
training dataset ends up defining the decision boundary.

And just to show how SVM can be used to do more interesting things
than finding a line between two sets of points, we show how by simply
swapping out the *linear kernel* (so whenever we compute the dot
product between two observations),

$$ K\left(x,x'\right) = \left\langle x, x' \right\rangle, $$

for a *radial kernel*, that is,

$$ K\left(x,x'\right) = \exp\left(-\gamma \left\|x-x'\right\|^2 \right), $$

we can use the same SVM routine to find a different set of support
vectors (defining the boundary of points from all sides), and a very
different classification boundary. Again, we will discuss how kernels
are relevant to SVM in a section below.

```{r}
rfit <- train(x, y, method="svmRadial")
rsv <- as.data.frame(x[rfit$finalModel@SVindex,])
grid <- expand.grid(x1=s,x2=s)
grid$y <- predict(rfit, newdata=grid)
grid$yy <- 2*(as.numeric(grid$y) - 1.5)
ggplot(dat, aes(x1,x2,col=y)) + geom_point() + 
  geom_point(data=rsv, col="black", size=5, shape=21) +
  geom_contour(data=grid, aes(x1,x2,z=yy), breaks=0, col="black") +
  geom_raster(data=grid, aes(x1,x2,fill=y), alpha=.2)
```

# Motivation behind the SVM solution

First we will give some motivation to how we solve for *w* and *b*
above. Again, supposing we have two linearly separable sets of points,
we want to find *w* and *b* so that the data are correctly classified,
that is, $w^T x - b \ge 1$ for all the data with $y=1$ and $w^T x - b \le
-1$ for all the data with $y=-1$. The distance between these two hyperplanes
is given by:

$$ \frac{(1 + b) - (-1 + b)}{\|w\|} = \frac{2}{\|w\|} $$

and so to make the margin as wide as possible corresponds to
minimizing $\|w\|$. The constrained optimization is then:

$$
\begin{aligned}
& \underset{w,b}{\text{min}}
& & \|w\| \\
& \text{s.t.}
& & w^T x_i - b \ge 1 : y_i = 1 \\
& & & w^T x_i - b \le -1 : y_i = -1
\end{aligned}
$$

Note that multiplying both of the constraints by $y_i$ then gives a
cleaner form:

$$
\begin{aligned}
& \underset{w,b}{\text{min}}
& & \|w\| \\
& \text{s.t.}
& & y_i(w^T x_i - b) \ge 1,\quad i=1,\dots,n \\
\end{aligned}
$$

And we can square the norm to make the optimization even easier,
because we will have a quadratic objective to minimize, and linear
constraints.

$$
\begin{aligned}
& \underset{w,b}{\text{min}}
& & \|w\|^2 \\
& \text{s.t.}
& & y_i(w^T x_i - b) \ge 1,\quad i=1,\dots,n \\
\end{aligned}
$$

# SVM objective solved with quadratic programming

We can take the above constrained optimization formulation and
directly plug it into a quadratic programming package to find the
optimal margin for the training data. The *quadprog* package in R
offers optimization for problems of the form:

$$
\begin{aligned}
& \underset{b}{\text{min}}
& & -d^T b + \tfrac{1}{2} b^T D b \\
& \text{s.t.}
& & A^T b \ge b_0 \\
\end{aligned}
$$

Unfortunately, they have used a *b* as well as the typically *b* that
is used in the SVM problem. We will refer to their *b* as
$b'$. Nevertheless, we can map our problem into their notation, by
setting $d=0$, $b' = [w,b]$, 
$D = \bigl(\begin{smallmatrix}I & 0 \\ 0 & 0\end{smallmatrix}\bigr)$,
$A^T = [y x^1, y x^2, \dots, y x^p, -y]$, and $b_0 =
[1,\dots,1]$. Here I have used $y x^j$ to refer to a column vector
where each $y_i$ is multiplied by sample i's value for the j-th
predictor, $x_i^j$.

Converting our SVM notation to the notation of *quadprog* gives:

```{r}
library(quadprog)
# min_w,b wT w s.t. y_i (w x_i - b) >= 1
# quadprog gives:
# min_b 1/2 bT D b s.t. AT b >= b0
yy <- 2 * (as.numeric(y) - 1.5) # {-1,1}
n <- length(y)
p <- ncol(x)
D <- matrix(0, nrow=p+1, ncol=p+1)
diag(D) <- 1
D[p+1,p+1] <- 1e-6
d <- numeric(p+1)
AT <- cbind(as.matrix(x), rep(-1, n))
A <- t(AT * yy)
b0 <- rep(1, n)
wb <- solve.QP(D,d,A,b0)
```

We can then pull out our fitted *w* and *b*, and plot them against the
training data:

```{r}
w <- wb$solution[1:p]
b <- wb$solution[p+1]
ggplot(dat, aes(x1,x2,col=y)) + geom_point() +
  geom_abline(intercept=(b+1)/w[2],slope=-w[1]/w[2],alpha=.2,linetype=2) +
  geom_abline(intercept=(b-1)/w[2],slope=-w[1]/w[2],alpha=.2,linetype=2) +
  geom_abline(intercept=b/w[2],slope=-w[1]/w[2],linetype=3)
```

# Non separable case

# Kernel trick

# References

* [Andrew Ng's notes on SVM](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
  
