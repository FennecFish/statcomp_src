---
title: "Random forests"
author: "Michael Love"
date: 12/14/2018
output: html_document
---

Download the
[APS Failure at Scania Trucks Data Set](https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks) from
the
[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)

The dataset description is:

> The datasets' positive class consists of component failures for a
> specific component of the APS system. The negative class consists of
> trucks with failures for components not related to the APS.
>
> The dataset consists of data collected from heavy Scania 
> trucks in everyday usage. The system in focus is the 
> Air Pressure system (APS) which generates pressurised 
> air that are utilized in various functions in a truck, 
> such as braking and gear changes. The datasets' 
> positive class consists of component failures 
> for a specific component of the APS system. 
> The negative class consists of trucks with failures 
> for components not related to the APS. The data consists 
> of a subset of all available data, selected by experts. 

The dataset has additional information associated with it. It is
mentioned that a false positive (cost 1) has a cost of 10, while a
false negative (cost 2) has a cost of 500. So false negatives are 50x
more costly as false positives.

> In this case Cost 1 refers to the cost that an unnessecary 
> check needs to be done by an mechanic at an workshop, while 
> Cost 2 refer to the cost of missing a faulty truck, 
> which may cause a breakdown. 

There is an imbalance in the dataset, such that there are 59x more
negative observations than positive observations:

> The training set contains 60000 examples in total in which 
> 59000 belong to the negative class and 1000 positive class. 
> The test set contains 16000 examples. 

There are 171 attributes

> The attribute names of the data have been anonymized for 
> proprietary reasons. It consists of both single numerical 
> counters and histograms consisting of bins with different 
> conditions. 
> ...
> The attributes are as follows: class, then 
> anonymized operational data. The operational data have 
> an identifier and a bin id, like 'Identifier_Bin'. 
> In total there are 171 attributes, of which 7 are 
> histogram variabels. Missing values are denoted by 'na'.

We also remove a troublesome line from the file using `grep`:

```
grep -v 8584297742 aps_failure_training_set.csv > clean.csv
```

```{r eval=FALSE}
library(readr)
dat <- read_csv("~/Downloads/clean.csv", skip=20, na="na")
table(dat$class)
table(sapply(dat[,-1], class))
summary(sapply(dat[,-1], function(dat) sum(is.na(dat))))
with(dat, boxplot(aa_000 ~ class))
with(dat, boxplot(ab_000 ~ class))
iddat <- c(which(dat$class == "pos"),
         sample(which(dat$class == "neg"), 1000))
dat2 <- dat[iddat,]
table(dat2$class)
with(dat2, boxplot(aa_000 ~ class))
by(dat2$aa_000, dat2$class, quantile, .5)
by(dat2$aa_000, dat2$class, quantile, .9)
(tab <- table(obs=dat2$class, pred=dat2$aa_000 > 75000))
tab["neg","TRUE"] * 10 + tab["pos","FALSE"] * 500
(tab <- table(obs=dat2$class, pred=dat2$aa_000 > 120000))
tab["neg","TRUE"] * 10 + tab["pos","FALSE"] * 500
```

```{r}
library(caret)
x <- as.data.frame(dat2[,2:20])
y <- dat2$class
x[is.na(x)] <- 0 # instead impute
trCtl <- trainControl(savePredictions=TRUE)
fit <- train(x, y, method="rf", trControl=trCtl)
table(fit$pred$mtry)
tab <- table(fit$pred$rowIndex[fit$pred$mtry==2])
head(tab,10)
barplot(table(tab))
# tg <- data.frame(mtry=c(5,10,15))
# fit <- train(x, y, method="rf", tuneGrid=tg)
fit$results
plot(fit)
fit$finalModel$importance
```

```{r}
library(rpart)
fit <- rpart(class ~ aa_000 + ag_004 + ag_005 + ah_000, data=dat2)
printcp(fit)
plotcp(fit)
summary(fit)
plot(fit); text(fit, use.n=TRUE, all=TRUE, cex=.8)
fit$cptable
(cp <- fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
pfit <- prune(fit, cp=cp)
plot(pfit); text(pfit, use.n=TRUE, all=TRUE, cex=.8)
```

```{r}
library(rpart)
dat <- data.frame(x=runif(1000))
dat$y <- sin(2 * pi * dat$x)
with(dat, plot(x,y))
fit <- rpart(y ~ x, data=dat, method="anova", cp=.001)
printcp(fit)
plotcp(fit)
pred <- predict(fit, dat)
with(dat, plot(x,y))
points(dat$x, pred, col="red")
```

```{r}
dat <- data.frame(x=runif(200))
dat$y <- sin(2 * pi * dat$x)
trCtl <- trainControl(method="cv", number=5, savePredictions=TRUE)
tg <- data.frame(mtry=1)
fit <- train(dat["x"], dat$y, method="rf", trControl=trCtl, tuneGrid=tg)
with(dat, plot(x,y))
pred <- fit$pred[order(fit$pred$rowIndex),]
points(dat$x, pred$pred, col="red")
```

```{r}
trCtl <- trainControl(method="cv", number=5, savePredictions=TRUE)
fit <- train(dat["x"], dat$y, method="knn", trControl=trCtl)
plot(fit)
with(dat, plot(x,y))
pred <- fit$pred[fit$pred$k == 5,]
pred <- pred[order(pred$rowIndex),]
points(dat$x, pred$pred, col="red")
```
