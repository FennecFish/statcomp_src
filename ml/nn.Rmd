---
title: "Neural networks"
author: "Michael Love"
date: 12/15/2018
output: html_document
---

This lecture note will give a brief introduction to (artificial)
neural networks (where we may include the word "artificial" to
contrast with biological neural networks). We begin by reviewing
logistic regression and the gradient descent/acsent method, and
connect this to the 
[perceptron](https://en.wikipedia.org/wiki/Perceptron).
We then describe how layers of perceptrons can lead to complex
prediction algorithms. Finally, we will construct various neural
networks for classifying the MNIST handwritten digits dataset. The
field of neural networks and deep learning is vast and quickly
evolving. For these interested in exploring the topic, I recommend
searching for current methods implementing deep learning for a
specific problem of interest.

This lecture note borrows from Andrew Ng's course notes for
[supervised learning](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
and the (offline) machine learning course notes
of [Guenther Walther](http://statweb.stanford.edu/~gwalther/).

```{r include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Gradient ascent for logistic regression

Recall that if we want to minimize least squares in a simple linear
model,

$$ \hat{y} = w_1 x_1 + w_2 x_2 + \dots w_p x_p + t, $$

we can use a gradient descent update rule for each predictor *j*,
either a *batch* update: 

$$ w_j^{t+1} = w_j^t + \lambda \textstyle \sum_{i=1}^N (y_i - \hat{y}_i) x_{ij}, $$ 

where we sum over all observations *i*, or *stochastic* gradient
descent where we update the $w_j$ by looking at one *i* at a time:

$$ w_j^{t+1} = w_j^t + \lambda (y_i - \hat{y}_i) x_{ij} $$ 

This is the gradient descent method, where we move in the direction
that minimizes the sum of squared error for each predictor, and where
the parameter $\lambda$ controls the rate of descent.

If we instead use the logistic function for a binary target,

$$ \hat{y} = f(w_1 x_1 + w_2 x_2 + \dots w_p x_p + t), $$

with a logistic function, *f*:

$$ f(z) = \frac{1}{1 + \exp(-z)}, $$

we have the same update rule for gradient *ascent* of the log likelihood
(here the stochastic version): 

$$ w_j^{t+1} = w_j^t + \lambda (y_i - \hat{y}_i) x_{ij} $$ 

# Perceptron

A limiting case of the logistic function is a function that only
outputs values of 0 or 1 around a critical value of $x = 0$:

```{r echo=FALSE, fig.width=5}
plot(function(x) 1/(1 + exp(-x)), xlim=c(-5,5), ylab="f(x)")
plot(function(x) 1/(1 + exp(-2*x)), xlim=c(-5,5), add=TRUE)
plot(function(x) 1/(1 + exp(-10*x)), xlim=c(-5,5), add=TRUE)
plot(function(x) ifelse(x >= 0, 1, 0), xlim=c(-5,5), col="red", add=TRUE)
```

If we use this step function instead of the logistic, the
classification algorithm is called the 
[perceptron](https://en.wikipedia.org/wiki/Perceptron).
The perceptron was invented in 1957 at the Cornell Aeronautical
Laboratory. From Wikipedia:

> The perceptron was intended to be a machine, rather than a program,
> and while its first implementation was in software for the IBM 704,
> it was subsequently implemented in custom-built hardware as the
> "Mark 1 perceptron". This machine was designed for image
> recognition: it had an array of 400 photocells, randomly connected
> to the "neurons". Weights were encoded in potentiometers, and weight
> updates during learning were performed by electric motors. 

For the perceptron, again we can use the update rule:

$$ w_j^{t+1} = w_j^t + \lambda (y_i - \hat{y}_i) x_{ij} $$

Although the update rules look very similar, the following note from
Andrew Ng's notes on supervised learning is apt:

> Note however that even though the perceptron may
> be cosmetically similar to the other algorithms we talked about, it
> is actually a very different type of algorithm than logistic regression and
> least squares linear regression; in particular, it is difficult to
> endow the perceptronâ€™s predictions with meaningful probabilistic
> interpretations, or derive the perceptron as a maximum likelihood
> estimation algorithm.

```{r}
set.seed(1)
n <- 100
x1 <- c(rnorm(n),rnorm(n,2))
x2 <- c(rnorm(n),rnorm(n,2))
x1 <- scale(x1)
x2 <- scale(x2)
y <- rep(0:1,each=n)
dat <- data.frame(y=factor(y),x1,x2)
library(ggplot2)
ggplot(dat, aes(x1,x2,shape=y)) + geom_point() +
n  scale_shape_discrete(solid=FALSE)
w <- c(-4,1)
t <- 3
w.mat <- matrix(nrow=6,ncol=2)
t.vec <- numeric(6)
lambda <- 0.01
for (i in 1:6) {
  w.mat[i,] <- w; t.vec[i] <- t
  y.hat <- ifelse(cbind(x1,x2) %*% w + t >= 0, 1, 0)
  for (j in 1:2) {
    w[j] <- w[j] + lambda * sum((y.hat - y) * x1)
  }
  t <- t + lambda * sum(y.hat - y)
}
w.mat[i,] <- w; t.vec[i] <- t
dat2 <- data.frame(t=t.vec, w1=w.mat[,1], w2=w.mat[,2], iter=1:6)
ggplot(dat, aes(x1,x2,shape=y)) + geom_point() +
  scale_shape_discrete(solid=FALSE) +
  geom_abline(data=dat2, aes(intercept=-t/w2, slope=-w1/w2, color=iter), size=1) +
  scale_color_continuous(low="green",high="blue")
```

# MNIST handwritten digits

* <http://varianceexplained.org/r/digit-eda/>
* <https://pjreddie.com/media/files/mnist_train.csv>

```{r}
library(readr)
mnist_raw <- read_csv("mnist_train.csv", col_names=FALSE)
dim(mnist_raw)
mnist_raw[1:5,1:5]
library(tidyr)
library(dplyr)
pixels_gathered <- mnist_raw %>%
  head(10000) %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
library(ggplot2)
sub <- pixels_gathered %>% filter(instance <= 12)
ggplot(sub, aes(x, y, fill = value)) + geom_tile() +
  facet_wrap(~ instance + label, labeller=label_both) +
  scale_fill_gradient(low="white", high="black")
```

```{r}
library(caret)
dat <- as.data.frame(mnist_raw[1:10000,-1])/255
y <- factor(mnist_raw$X1[1:10000])
idx <- y %in% c("3","5","8")
dat <- dat[idx,]
y <- droplevels(y[idx])
pc <- prcomp(dat)
plot(pc$sdev[1:20]^2/sum(pc$sdev^2))
x <- pc$x[,1:7]
boxplot(x[,1] ~ y)
ggplot(data.frame(x,y), aes(PC1, PC2, col=y)) + geom_point()
table(y)
tg <- data.frame(k=c(5,9,13,17,21,25))
trCtl <- trainControl(savePredictions=TRUE)
fit <- train(x, y, method="knn", tuneGrid=tg, trControl=trCtl)
ggplot(fit, metric="Kappa")
fit$results
tab <- table(obs=fit$pred$obs, pred=fit$pred$pred)
prop <- tab/rowSums(tab)
round(prop, 3) * 100 # percent
```

```{r}
plotWithSD <- function(fit, param, a=2) {
  min <- with(fit$results, min(Kappa - (a+2)*KappaSD))
  max <- with(fit$results, max(Kappa + (a+2)*KappaSD))
  fit$results$ymax <- with(fit$results, Kappa + a*KappaSD)
  fit$results$ymin <- with(fit$results, Kappa - a*KappaSD)
  ggplot(fit$results, aes_string(param, "Kappa", ymax="ymax", ymin="ymin")) +
    geom_ribbon(fill="black", alpha=.1) +
    geom_point(color="blue") + geom_line(color="blue") +
    ylim(min,max)
}
plotWithSD(fit, "k")
```

```{r}
dat <- as.data.frame(mnist_raw[1:10000,-1])/255
y <- factor(mnist_raw$X1[1:10000])
pc <- prcomp(dat)
x <- pc$x[,1:10]
boxplot(x[,1] ~ y)
ggplot(data.frame(x,y), aes(PC1, PC2, col=y)) + geom_point()
tg <- data.frame(k=c(5,9,13,17,21,25))
trCtl <- trainControl(savePredictions=TRUE)
fit.knn <- train(x, y, method="knn", tuneGrid=tg, trControl=trCtl)
fit.knn$results # 88% Kappa
plotWithSD(fit.knn, "k")
```

```{r}
tab <- table(obs=fit.knn$pred$obs, pred=fit.knn$pred$pred)
prop <- tab/rowSums(tab)
round(prop,3)*100 # percent
diag(prop) <- NA
```

```{r, fig.width=5, fig.height=5.5}
image(prop, xaxt="n", yaxt="n",
      xlab="obs", ylab="pred",
      col=colorRampPalette(c("white","blue"))(50))
for (i in 1:2) axis(i, 0:9/9, 0:9)
abline(0,1,col="red")
mxs <- head(sort(prop,decreasing=TRUE),3)
lbl <- 100*round(mxs,3)
arr.idx <- sapply(mxs, function(i) which(prop == i, arr.ind=TRUE)) - 1
text(arr.idx[1,]/9, arr.idx[2,]/9, labels=lbl, cex=.9, col="white")
```

```{r eval=FALSE}
x <- pc$x[,1:20]
tg <- data.frame(C=c(.5,1,2))
# 700 seconds
system.time({
  fit <- train(x, y, method="svmRadial", trControl=trCtl, tuneGrid=tg)
})
plotWithSD(fit, param="C")
fit$results # 10k data, 20 PCs, 94.5% Kappa
```

```{r eval=FALSE}
x <- pc$x[,1:60]
tg <- data.frame(size=60)
trCtl <- trainControl(method="cv", number=5, savePredictions=TRUE, verboseIter=TRUE)
# 60 s
system.time({
  fit <- train(x, y, method="mlp", trControl=trCtl, tuneGrid=tg, maxit=50)
})
fit$results # 93.8% Kappa
```
