---
title: "constrained"
author: "Naim Rashid"
date: "12/5/2018"
output: 
  html_document:
    number_sections: true
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \Y \bfm{Y}$'
- '$\def \X \bfm{X}$'
- '$\def \z \bfm{z}$'
- '$\def \betab \bdm{\beta}$'
- '$\def \Omegab \bdm{\Omega}$'
- '$\def \pib \bdm{\pi}$'
- '$\def \thetab \bdm{\theta}$'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#  Introduction

In the previous lectures we talked about approaches that seek to optimize particular functions with respect to a given set of parameters.  Such functions could be simple with only a single parameter, or complex such as a likelihood function with multiple unknown parameters.

In this lecture we will connect some of the optimization approaches discussed earlier with alternative approaches such as Linear Programming and Quadratic Programming.  We will see that some of the problems we described earlier can be reformulated into and unconstrained Linear or Quadratic programming problem.  The reason for making such a connection is that once formulated into these settings, we may apply standard and general off the shelf solvers to obtain parameter estimates.  

We will discuss both the unconstrained and constrained optimization setting, where in the latter one often places some sort of constraint on the parameters to be optimized over.  There are various types of constaints that one may select, and again reformulating one's problem may allow for the application of avaliable solvers for constrained Linear/Quadratic Programming problems.  

From here, we will segway into penalized likelhood estimation and show that in certain cases we may use Linear/Quadratic programming to similarly solve such constrained maximization problems.   In the regression setting, penalized likelihood estimation is often used for the purposes of variable selection in high dimensional settings.  We will discuss several procedures in the literature for performing maximization in such settings, and talk about efficient implementations.  We will also discuss the impact of the choice of penalty.

As a side note, there is a rich literature on Linear and Quadratic Programming topics stemming back many decades, however we will only cover a small portion of this topic related to the upcoming topic of Support Vector Machines (SVMs) in Module 3 of this course.  We will also cover topics that are relevant to previously discussed statistical estimation problems and penalized likelihood estimations.  The discussion on variable selection via penalized likelihood will connect with the material presented in the chapter 4 of Bios 761.

# Unconstrained Optimization

The term "unconstrained"  in Unconstrained Optimization relates to the parameters in the function that are being optimized over.  That is, no bounds or limits are being placed on the parameters or functions of the parameters when trying to minimize the objective function of interest.   

To introduce the topic, let us start with a familiar problem.  In the first lecture, we talked about maximum likelihood estimation in the context of linear regression.  In this setting, let us assume that we have an $n\times 1$ vector of observed responses $\y$ and an $n \times p$ full rank matrix of predictors $\X$.  We assume that $$\y = \X\betab + \epsilonb,$$ where $\betab$ is a $p \times 1$ vector of unknown parameters to be estimated, $\epsilonb$ is an $n \times 1$ vector of unobserved errors such that $\epsilon_i\sim N(0,\sigma^2)$ for $i = 1\ldots n$, and $\sigma^2$ is the variance of each of the unobserved errors (also unknown).  In doing so, we assume that the relationship between $\y$ and $\X\beta$ is linear, with some error.

In our first lecture, we discussed for the intercept-only model case how one may perform maximum likelihood estimation to obtain estimates for $\betab$.  We can also show how this approach is equivalent to the problem of minimizing the regression sum of squares $\sum_{i = 1}^n (y_i - \X_i\betab)^2$, where $\X_i$ is the set of covariates pertaining to the $i'th$ subject.  We also may write this as $(\y - \X\betab)'(\y - \X\betab)$.  There is a close form for the minimized of the RSS, and during the derivation of which we arrive at the normal equations $\X'\X\betab = \X'\y$.  This implies that $\hat{\betab} = (\X'\X)^{-1}\X'\y$, our closed form solution for the minimizer of the RSS.  

Recall that we do not explicitly make any assumptions regarding $\epsilonb$ when decide to obtain $\hat{\betab}$ through simply minimizing the RSS.  If we make the additional usual assumption that $\epsilon_i\sim N(0,\sigma^2)$ for $i = 1\ldots n$, then we know that this minimizer is also the UMVUE estimator for $\beta$.  This assumption is implicit when we obtain $\hat{\beta}$ instead through maximum likelihood estimation.

Alternatively, we may use unconstrained quadratic programming to obtain the minimizer of the RSS.  

Let us write the general form of an Unconstrained Quadratic Programming problem as the following:

Minimize $||Ax - b||_2^2$ = \x'\A'\Ax - 2\b'\A\x + \b'\b$ over $\x$

For the current regression example, $\A = \X$, $\x = \betab$, and $\b = y$.  Note that $||Ax - b||_2^2 = ||b - Ax||_2^2$ and that we do not put any explicit bounds on $\x$.  With no surprise, we can derive the minimizer of this objective function in a manner very similar to our minimizer for the RSS.  That is, we have an analytic solution to this problem $\hat{x} = (\A'\A)^{-1}\A'\b$.  If we assume that $\A$ is full rank, then this implies that this solution (as in the regression case) is unique. 

Generally speaking, we can use the methods for lecture 1 to perform optimization in this setting.  So why bother introduce this notation?  We do this to set up the unconstrained setting in the next section, where such objective functions are no longer convex. In general, we rarely see many unconstrained versions of Linear Programming problems as the objective function is linear, and therefore does not have a natural minimum without constraints as in the quadratic case.  That is, optimization in the quadratic case may be convex and therefore may naturally have a unique solution in the absence of constraints, but this is not the case with the linear objective function.  We will show examples of linear programming later in this lecture. 

# Constrained Quadratic Optimization (Quadratic Programming)

Now lets move to the case where 
