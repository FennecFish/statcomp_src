---
title: "em"
author: "Naim Rashid"
date: "10/26/2018"
output: 
  html_document:
    number_sections: true
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \Y \bfm{Y}$'
- '$\def \X \bfm{X}$'
- '$\def \z \bfm{z}$'
- '$\def \betab \bdm{\beta}$'
- '$\def \Omegab \bdm{\Omega}$'
- '$\def \pib \bdm{\pi}$'
- '$\def \thetab \bdm{\theta}$'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Reading:  GH Chapter 4, McCullough Chapter 1

The EM algorithm is a general-purpose optimization algorithm widely considered as a "workhorse" computing algorithm in statistics.  While it was originally introduced for the purpose of optimization in the presence of missing data, we will see that it can also be adapted to broader problems in statistics with a reformulation of the original problem at hand.  In such settings, the EM algorithm may offer a convenient alternative to the methods introduced in the previous lecture where the analytical forms of the derivatives may be complex or difficult to evaluate.  

Examples of these broader applications include random effects, finite mixture, hidden markov, and latent class models. The relative ease of its implementation in such complex models is also an attractive feature. 

## Example:  Finite Mixture Models

### Newton Raphson

### BFGS

### Nelder-Mead

## The Basic Idea
As we can see from the previous example that applying methods such as NR to maximize the finite likelihood can be somewhat tedious, due to the form of the derivatives.  The general intuition behind the EM algorithm involves the maximization of a surrogate function in lieu of the original function/likelihood, which may be more amenable to maximization with standard approaches such as NR or BFGS.  

In this manner, the problem at hand is transformed from a missing or "incomplete" data problem to a "complete" data problem, where the missing data is assumed to be known.  

That is, assuming the missing data to be known reduces the complexity of the maximization problem and often times has a much nicer form.  In cases where there is no actual "missing" data in the original likelihood (like in the finite mixure model example), we may introduce some missing data to make it amenable to maximize via EM.  

But how does one actually maximize such a complete data model when obviously there is no way to know the actual values of the missing data?  The answer to this reflects the simplicity of the EM algorithm, and also explains how other disciplines had arrived at similar algorithms (albeit without any formal justification for its performance) prior to the seminal 1977 publication on the EM algorithm by Dempster, Laird, and Rubin (DLR).  

## Algorithm Strategy
The surrogate function being maximized is the **expectation** of the **complete data log likelihood** with respect to the "missing" or "latent" data, conditional on the observed data and the current estimates of the model parameters.  This function is often simpler in form than the original log likelihood, and is more amenable to maximization.  We will go into more detail on the CDLL in the next section. 

The EM algorithm alternates between two main steps, the "Expectation step" or "E-step", and the "Maximization step" or "M-step".  In a very general sense, during EM we "fill in" the missing data with an educated guess  (E step), and then maximize the complete data log likelihood with the missing data "filled in" using standard optimization methods (M-step). We iterate between the E and M steps until model convergence.  

This way, we do not directly deal with missing data in the M-step, facilitating the application of existing maximization routines in the M-step. DLR showed that the maximization of this function leads to the maximization of the likelihood, and each iteration is guarunteed to have a non-decreasing change in the likelihood. 

In this lecture we will first start with the formulation of the EM approach, its general properties, variants of this approach, and finally finish with some examples. 

# Algorithm Setup

## General formulation
Let $\Y$ be the $n$-dimesional random vector pertaining to the vector of the observed data $\y_o$. Let us assume that $\Y$ is distributed with PDF $g(\y ; \thetab)$, where $\thetab = (\theta_1,\ldots,\theta_p)$ is a $d$-dimensional vector of unknown parameters to be estimated and $\thetab \in \Omegab$, $\Omegab$ being some $d$-dimesional space for $\thetab$.  

To be clear, we are treating $\y_o$ here in the general sense in that it pertains to all the **observed data** for a particular model.  This is exactly how it sounds like, in that it pertains to the data that we can actually collect and have on hand in our problem.  An example of this may be the observed overall survival times for a subset of cancer patients in a clinical trial. 

As mentioned earlier, the EM algorithm is helpful in situations where there is missing data.  In other situations, it may be helpful to reformulate a problem with no missing data into a missing data problem.  This may be done, for example, by introducing a latent variable that may simplify the likelihood and thus computation.  Such latent variables may be considered as hypothetical and never observable in some sense, but as we will see later, leads to nice for of the complete data likelihood that is suitable for maximization.  

In either case, we may term the observed data $\y_o$ as the "incomplete data" in this setting, and the "complete" or "augmented" data as as $\y_c$, where $y_c = (\y_o^T, \z^T)^T$, where $\z$ pertains to the vector of missing or unobservable data. 

### Simple examples of "missing" data in this context
Examples of $\z$ in the former case may pertain to the actual (unobserved) survival times of patients in the study who were censored in the clinical trial. For example, those patients who did not pass away by the end of the study or were lost to follow up. We only observe their survival time up their last followup ($y_o$). 

However, if we wait long enough, we should be able to observe the survival times for all patients in the trial, without any censoring.  Or, if we were able to track down those patients that were lost to followup, we could observe how long they actually had survived. Here, the data is truly missing in that in some scenarios there is a possibility that we can observe the data. 

Examples of $\z$ in the latter case may pertain to the set of class memberships in a finite mixture model, or state-membership in a hidden markov model.  Unlike the survival example, there is no possibility to observe such states in reality. 

Assuming that each observation in the model belongs to a single class or a single state greatly simplifies the problem and facilitates the maximization of the model.  We will give examples of these types of models in a bit.  



### Defining the Complete Data Log Likelihood

Let us assume that the distribution for the random vector $Y_c$ pertaining to the complete data vector $\y_c$ is given by the pdf $g(\y_c ; \thetab)$.  Given this setup, we can define the *complete data log likelihood* function as $$log L_c(\theta_b) = g(\y_c; \thetab).$$  From this, it is clear that the likelihood can be simply obtained by integrating out the missing data from the complete data likelihood, such that $$g(y_o,\theta_b) = \int g(\y_c; \thetab)d\z $$  

### Q-function and Initialization

The objective function to be maximized over can be considered a surrogate function of the likelihood,  termed the "Q-function".  This function is defined at the $k$th step as $$Q(\thetab,\thetab^{(k)}) = E\left[ log L_c(\thetab) | \y_o,\thetab^{(k)}\right],$$ the expection of  the complete data log likelihood with respect to the missing data $\z$, given the observed data and the current value of the parameter estimates. 

Similar to the algorithms introduced in the prior lecture, the EM algorithm is iterative and begins at some starting value for $\theta$ which we denote as $\thetab^{(0)}$.  Better starting values may result in faster convergence, as well as higher chance of converging to the global maximum as opposed to a local one.  We then proceed to the E-step in the next section. 

**Alternatively**, in some cases it may make sense to start with an initial value of the E-step, and then proceed to the M-step.  We will give an example of this situation later in the finite mixture model example. 

### E-step
In the E-step, the expected value of the complete data likelihood is updated given the current value of the parameter estimates and the observed data. In some sense, we fill in the missing data in the complete data log likelihood with their expected values at that step, given the observed data and current parameter estimates from the M-step.  This approach can be thought of a way of substituting an "educated guess" for their unknown values to simplify the application of the methods from the previous lecture.  

Now, in many examples this analogy of "filling in" the missing data with their conditional expection may hold, however in a more general sense we are computing the conditional expectation of the complete data log likelihood with respect to the missing data, given the observed data at the current parameter estimates. 

For example, if say the missing data in a problem was some variable $q$, and in the complete data log likelihood both the terms $q$ and $q^2$ appeared, $E[Q^2| \y, \thetab] \neq E[q, \y, \thetab]^2$. That is, we cannot simply "fill in" the guess for $q^2$ in the CDLL with the square of the guess for $q$.  We will still have to formally evaluate the conditional expection for $q^2$ and fill the value in for that particular terms. 

Another way to think about this is that we are integrating out the missing data from the complete data log likelihood weighted by the posterior distribution of the missing data, conditional on the observed data and current parameter estimates.  

If complex functions of the missing data are present in the CDLL this can complicate the evaluation of this integral, and hence E-step.  Luckily, in most cases this integral (and thus expectation), can reduce to simple forms.  We will show examples of both common simple cases and more complex ones, and how one can address the latter cases with extensions of the EM algorithm later in this lecture.

In the E-step at iteration $k$, we calculate  $Q(\thetab,\thetab^{(k)})$, where $Q(\thetab,\thetab^{(k)}) = E\left[ log L_c(\thetab) | \y_o,\thetab^{(k)}\right].$  Therefore, after starting at an initial value $\thetab^{(0)}$, we can proceed to the E-step and begin the algorithm.  

### M-step
At step $k$, the M-step  maximizes the Q-function $Q(\thetab,\thetab^{(k)})$ with respect to $\thetab$ over the parameter space $\Omegab$.  In other words, $\thetab^{(k+1)}$ is chosen such that $Q(\thetab^{(k+1)},\thetab^{(k)}) > Q(\thetab,\thetab^{(k)}) \forall \thetab \in \Omegab$.

Optimization methods such as those discussed in the prior lecture are now applicable in the M-step, simplifying the optimization procedure relative to before.  In this sense, the EM algorithm is modular, where one can apply existing maximization procedures to maximize the Q-function even in situations where the likelihood is quite complicated, for example necessitating the evaluation of multidimensional integrals or require recursive computation.  

### Convergence Criteria
The algorithm iterates between the E and M steps until the value of the Q-function or parameter estimates converge.  The same principles regarding choosing informative starting points and convergence criteria apply to the EM algorithm as well.  We will see that some of the properties of the EM algorithm enables it to be quite robust regardless of setting and selected starting points, but can be slower to converge relative to other methods.  It is also not immune to converging to local maxima.  

### General comments
The 1977 paper on the EM algorithm demonstrated that the observed or incomplete data likelihood function (referred to as the likelihood function in other contexts) is guarunteed not to decreases with each EM iteration such that $L(\thetab^{(k+1)}) \leq L(\thetab^{(k)})$.  This is an attractive property as each iteration should improve the likelihood in some sense.  As can be seen above, the algorithm itself is quite modular, where simpler existing methods can be applied to evaluate the E and M-step.  In the E-step only simply needs to know the condition density of the missing data given the observed data.  In cases where this density is unknown or intractable, approaches such as the monte-carlo EM may be utilized to approximate the E-step using sampling-based approaches. 

## Examples (TO DO)
So we introduced a fair bit of notation here, and to really illustrate how this approach works lets start with two simple examples. 

In the first example, we show a case where we reformulate a problem by introducing a latent variable that allows for the maximization via EM and simplifies the problem.  

In the second example, we illustrate an application of EM to a missing data scenario that is naturally seen in survival analysis.  

In each case, we will break down the application of the EM algorithm using the same structure introduced above:

*    Define the log likelihood (observed data log likelihood)
*    Define the Complete Data Log Likelihood 
*    Derive the Q-function
*    Compute the E-step
*    Compute the M-step
*    Define intialization and convergence criteria

### EM application to the Finite Mixture Model Example

## Pros and Cons of EM
* Pros
    + Numerically stable, each iteration increases likelihood
    + Reliable global convergence, depending on starting point
    + Easy to implement, modular
    + Avoids direct evaluation of likelihood and derivatives of likelihood
    + In general memory efficient (does not need to store information matrix or its inverse)
    + M-step can be maximized with standard packages or simplified using extensions (ECM, etc) if needed
* Cons
    + No direct way to obtain covariance matrix of parameter estimates as can be done with NR (strategies to do this later)
    + Slow convergence, especially when there is a lot of missing information
    + Does not guarantee convergence to the global maximia if multiple maxima are present, but this is the case for other approaches as well. 
    + In some cases the E-step may be difficult to evaluate or intractable (for example when the evaluation of multidimensional integrals are needed).  The MCEM algorithm that uses Monte Carlo sampling to approximate the E-step is one way around this. 


# General Properties of the EM Algorithm

In this section we will not focus on the proofs behind the results shown but instead highlight some of their results and their implications on the properties of the EM.  

## Monotone Increase of the Likelihood

In the initial DLR paper on the EM, it was demonstrated that $$L(\thetab^{(k+1)}) \geq  L(\thetab^{(k)})$$ for each iteration $k \geq 0$, demonstrating that the likelihood at each iteration of the EM does not decrease. Therefore, for some bounded sequence of likelihood values ${L(\thetab^{(k+1)})}$, we know that the likelihood at each iteration converges to some likelihood value $L^*$, which may or may be either a local or global maximum (see prior discussion).

The "self-consistency" property comes from this result, where we can show that if the MLE $\hat{\thetab}$ is the global maximizer for the likelihood, this implies that it is also the global maximizer for the Q-function (the surrogate for the likelihood).  In order words, if $\hat{\thetab}$ is the global maximizer for $L(\thetab)$, then this implies that $$Q(\hat{\thetab},\hat{\thetab}) \geq Q(\thetab,\hat{\thetab})$$ and that $\hat{\thetab}$ is the solution to the equation $$\frac{d}{d\thetab} Q(\thetab\hat{\thetab})\|_{\thetab = \hat{\thetab}} = 0.$$  The latter statement translates to that the optimal value for $\thetab$ for the likelihood is also the optimal value for the Q-function.

In a more general sense, $\thetab^{(k+1)}$ is chosen to maximize $Q(\thetab,\thetab^{(k)})$ at iteration $k$ rather than globally.  Using similar reasoning for the prior result, we can show that this results in a sequence of estimates across iterations satisfying $L(\thetab^{(k+1)}) \leq L(\thetab^{(k)})$, and therefore the likelihood will not decrease as the algorithm progresses. 

Under certain conditions, such as if the likelihood function is unimodal in $\thetab$ (one global maximum with no local maximums), then any sequence ${\thetab^{(k)}}$ will converge to the unique MLE of $\thetab$, $\thetab^*$.  In cases where the parameter space is not unrestricted and may be constrained, such covergence may not be guarunteed but depends on the specific scenario. 

## Convergence Rates THIS NEEDS TO BE UPDATED
Using similar arguments at the last lecture, we can use a Taylor Series Expansion of $\thetab^{(k)}$ around $\thetab^{(k)}$.  We can show that around $\thetab^*$, the rate of convergence of the EM is approximately linear, however the exact rate depends on the problem at hand.  

Theoretically, the rate of convergence can be determined from the eigen,alues of the Jacobian Matrix at $\theta^*$, where the component with the largest eignevalues (indicating the slowest rate) determines the overall rate of convergence. 

## Computing Standard Errors of Parameter Estimates
For the NR algorithm, the hessian matrix is estimated with each update of the algorithm, and one nice byproduct of this is that standard errors can be directly computed for the parameter estimates after convergence using the hessian.  For methods such as BFGS where no hessian is computed, its approximation (often used during maximization) can similarly be utilized for standard error computation.  That is, the covariance matrix of the parameter estimates can be determined with relative ease. 

For the EM algorithm, due to the surrogate function being used and the modularity of the approach, we do not have a ready made hessian matrix as in the previous methods.  As a result, quantities such as standard errors are harder to obtain.  However, several approaches exist to obtain such estimates post-convergence.  

Methods for direct evaluation of the covariance matrix have been developed, but in some cases may be tedious to derive and implement.  Such approaches require one to derive the second derivatives of the likelihood in question, which may have been one of the reasons why the EM was chosen in the first place to avoid doing.  After convergence of the EM algorithm, the MLE is utilized to calculate the observed information matrix from these forms, which is then inverted to obtain the standard errors. We will not go into too much detail on this topic, but further information on this approach in given in McCullough Section 4.2.2, and an approximation for the i.i.d. case is given in Section 4.3, and for non-i.i.d data in 4.5.  Several specific algorithms for implementing these direct calculation approaches are given in McCullough Section 4.7

A very general and popular approach that can be applied to all models is the bootstrap.  There are multiple versions of this approach, however one approach commonly used is the parametric bootstap.  Essentially, one first fits the model to the data to obtain $\hat{\thetab}$, and then simulates generates $B$ bootstrap datasets of size $n$ from the fitted model.    For each of the $B$ datasets, we refit the model and save the parameter estimates.  Then, for the purpose of covariance matrix estimation for the model parameters, we simply calculate the covariance of the parameter estimates from each of the $B$ fitted models. 

An alternative version is the nonparametric bootstrap, where instead of simulating $B$ bootstrap samples from the fitted model, we instead resample with replacement from the original dataset to get out $B$ bootstrap datasets.  Then, similar to the parameteric approach, we fit our model on each of the $B$ bootstrap datasets, and then calculate the covariate matrix of the parameter estimates across the cases. The nonparametric version may be preferred in cases where we have "true missing" data, as indicated prior in the lecture, for example in cases where we have censoring in our dataset.  In such cases it is difficult to know and simulate the mechanism and factors driving the missingness in the data, and resampling the original data is one robust way to avoid have to make assumptions on that when trying to simluate the data from a fitted model. 

### Example of bootstrap evaluation of standard error for estimated parameters

# Extensions of the EM Algorithm
Since the initial DLR paper, several extensions of the EM algorithm have been published.  Each approach address potential issues and shortcomings of the EM algorithm in certain settings and aim to speed up its application or get around intractable numerical problems.  As we will see, these modifications may occur in the M-step, where the maximization procedure can be simplified through various approaches, or in the E-step, where the conditional expectation may be difficult or impossible to evaluate analytically. 

## Expectation Condition Maximization (ECM, M-step Modification)



### Why should we consider

In some cases the M-step may be computationally complex or difficult to evaluate.  For example in cases when many parameters exist, or when the off-the-shelf maximization routine for the M-step is computationally intense of difficult to implement.  

One solution to this problem is that instead of maximizing all of the model parameters simultanously, we instead maximize each parameter, or groups or parameters, in sequence, condition on the prior values in the M-step.  In contrast to the regular EM algorithm, we call this approach Expection Conditional Maximization (ECM), as in the M step is now a series of sequential updates in terms of the parameters.  These maximizations may require iterative approaches, such as those discussed in the previous lecture, or have closed forms. 

We will see that in many cases this approach may result in more iterations of the EM algorithm, but less overall runtime, as the amount of time spent on each M-step is less. Other benefits include greater stability in maximizing over a simpler parameter space in each CM step.  

### Formulation
Let us now consider that we replace the original M-step with $S$ CM steps.  As we mentioned before, this may pertain to the maximization of $S$ individual parameters or $S$ groups of parameters.  

Then, let us define $\thetab^{(k+s/S)}$ as the value of $\thetab$ on the $s$th step of the M-step. Then, at each CM step $s$, we maximize $Q(\thetab,\thetab^{(k+(s-1)/S)})$ defined as the Q-function at the $k$th step following the $(s-1)$th conditional maximization step  with respect to $\thetab$.  
### Speed of Convergence
It has been shown that the ECM reduces to a simple CM step in the absence of missing data.  Prior work has shown that if a CM approach is expected to converge in the absence of missing data in a model, then we should expect similar convergence for an ECM approach 

It has been shown that the total speed of convergence is simply $$s_{ECM} = s_{EM}s_{CM},$$ reflecting the product of the speed of the convergence of the regular EM and the CM step of the function in question in absence of missing data.  Then, it makes sense that $s_{EM}s_{CM} < s_{ECM} < s_{EM}$, where the speed on convergence of the ECM is slower than the EM (in terms of the number of iterations), as the Q-function is being improved conditionally only bit by bit in the M-step of the ECM.  However, given that the each CM in the M-step is much faster than the standard M-step in the EM algorithm, the total time it takes for all $S$ CM steps to complete may be much faster.  

Therefore, while the ECM may **take more iterations to converge than EM**, the faster and simper CM steps may result in **lower overall run time**, as the M-step will be faster to execture in each iteration. This is reflective of the main reason why one would pursue such an approach over standard EM when the M-step is complicated.  

## Multicycle ECM 

This variant is an extension of the ECM algorithm above, except that we perform an E-step update after each CM step.  It can be shown that the ECM retains similar properties as the ECM algorithm in terms of convergence and the ascent property after each EM iteration.  However, in some cases we may see a larger increase in the likelihood per EM iteration due to the Q function being updated more often.  However, this may not always be the case and in some cases the algorithm may converge more slowly than the ECM.

In general, this approach is best applied when the E-step computation is very quick and simple, as the additional computational burdern of multiple E-step evaluations per EM iterations will be relatively low. 


## Example

## Monte Carlo EM (MCEM)

### Why should we consider
In some cases, the E-step may be analytially or computationally intractable.  For example, the expectation does not have a clear closed form (unlike the examples given prior), or hard to evaluate (for example involving multidimensional intergrals).

To mitigate this, we replace the expectation in the E-step with an approximation using a "Monte Carlo" E-step, yielding the term MCEM or Monte Carlo EM.  


### Formulation

In a general sense, the modification over the standard EM approach is largely in the E-step, where we approximate the expecation using monte carlo sample-based approaches.  The M-step then maximizes the CDLL with the missing data filled in with these $M$ drawn samples, weighting each filled-in case by $1/M$.  In other words, we are essentially maximizing the M-step as in the regular EM case, except we are averaging over each drawn sample.  This will become clear in the setup below.  

We can rewrite the expectation of the Q-function as an integral, where the integrand can be factored into two parts: $$ Q(\thetab,\thetab^{(k)}) = E\left[ log L_c(\thetab) | \y_o,\thetab^{(k)}\right] = \int g(\y_c; \thetab)d\z = \int f(\y_o; \thetab^{(k)}, z) + f(z ; \thetab^{(k)}, y_o) d\z .$$

That is, the conditional expection of the CDLL can be factored into the form above, where the first part is the pdf of the observed data given the missing data and current parameter estimates. The second part pertains to the posterior distribution of the missing data given the observed data and current parameter estimates.  

As one can imagine, this intergral may be difficult to evaluate analytically, especially in cases where $g(\y_c; \thetab)$ does not simplify to some known distribution (and thus would have an known closed form for the integral).  In addition, if the dimension of $z$ is large, it may be difficult to evaluate multiple integrals, expecially if no closed forms for evaluating the integrals exist.  

One way to avoid this is issue is to approximate the integral by using numerical integration.  There are multiple approaches to do this, and we cover these approaches in a later lecture.  One flexible approach is to use monte-carlo integration, where if we can say draw $M$ samples from $f(z ; \thetab^{(k)} , y_o)$ $z^{1}\ldots z^{M}$, then it can be shown that $\int f(\y_o; \thetab^{(k)}, z)f(z ; \thetab^{(k)}, y_o) d\z = \frac{1}{M}\sum_{m = 1}^{M}f(\y_o; \thetab^{(k)}, z^{m})$.  That is, we approximate the integral by drawing many samples from the posterior distribution of the missing data at step $k$, and then average over $f(\y_o; \thetab^{(k)}, z^{m})$, filling in the missing data with each draw (weighted by $1/M$).  

Therefore, in the E-step, we can approximate the Q-function using the approach above.  In application, the majority of the effort in the E-step is drawing samples from $f(z ; \thetab^{(k)} , y_o$, which also may not have a good closed form itself.  If we do not know what the form of this conditional distribution is, how exactly can we draw samples from it?  We cover several of these approaches in our lecture on Numerical Integration and MCMC.  For now, you just need to know that we can sample from this posterior in each step of the MCEM algorithm. 

One thing that is clear in this case is that the larger the value of $M$ is, the greater accuracy we will have in evaluating this expectation.  Therein lies one of the weaknesses of this approach - many samples may need to be drawn, which increases the computational burden of this approach.  The procedure for obtaining samples from this distribution through monte carlo approaches may also be computationally expensive as well (rejection sampling, metropolis-hastings, etc).  
In addition, a large number of samples may also be needed for convergence.  Given that the E-step is being approximated, and that this approximation is based upon draws of random (or approximately random in some cases) samples, the Likelihood, Q-function, or model parameters may vary randomly about some value for smaller values of $M$ even if the model has truly converged.  Increasing M descreases the monte carlo error of these values, and the random variability decreases as well, increases your chances of convergence.  Some authors have put forward approaches for estimating this monte-carlo error at each step to determine by how much we may need to increase $M$ by at each step of the MCEM algorithm to facilitate eventual convergence.  Others simply increase $M$ by a predictable amount each step.  In either case, we would like to use a smaller $M$ a the beginning of the MCEM algorithm and a larger $M$ closer to convergence to ensure the model convergences.

Various convergence criteria as defined earlier can be used, but given the random variability in the parameter estimates, likelihood, or Q-function in this case, oftentime one may simply terminate the algorithm if the convergence criteria has been met say three times in a row so that it is likely not due to random chance. 


### Example
To illustrate this application, we use an example from fitting Generalized Linear Mixed Models (GLMMs).  Without loss of generality, let us assume that we are assuming our response $\y$ is binary, and we wish to model the response with some set of predictors $\X$, where the effect of some of these predictions may vary across subjects.  Let us assume here that $\X$ is an $n \times 4$ matrix, pertaining to an intercept and three predictors whose effects we assume to vary randomly across subjects.  Therefore, we can utilize the random effects logistic regression model to fit such a model.

## Rejection-controlled EM

### Formulation

### Why should we consider

## Other Versions

* ECME
* Alternating ECM   

# Other applications

## Finite Mixture Modeling
  
## Hidden Markov Models




