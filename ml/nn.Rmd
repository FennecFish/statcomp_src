---
title: "Neural networks"
author: "Michael Love"
date: 12/15/2018
output: html_document
---

This lecture note will give a brief introduction to (artificial)
neural networks (where we may include the word "artificial" to
contrast with biological neural networks). We begin by reviewing
logistic regression and the gradient descent/acsent method, and
connect this to the 
[perceptron](https://en.wikipedia.org/wiki/Perceptron).
We then describe how layers of perceptrons can lead to complex
prediction algorithms. Finally, we will construct various neural
networks for classifying the MNIST handwritten digits dataset. The
field of neural networks and deep learning is vast and quickly
evolving. For these interested in exploring the topic, I recommend
searching for current methods implementing deep learning for a
specific problem of interest.

This lecture note borrows from Andrew Ng's course notes for
[supervised learning](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
and the (offline) machine learning course notes
of [Guenther Walther](http://statweb.stanford.edu/~gwalther/).

```{r include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Gradient ascent for logistic regression

Recall that if we want to minimize least squares in a simple linear
model,

$$ \hat{y} = w_1 x_1 + w_2 x_2 + \dots w_p x_p + t, $$

we can use a gradient descent update rule for each predictor *j*,
either a *batch* update: 

$$ w_j^{t+1} = w_j^t + \lambda \textstyle \sum_{i=1}^N (y_i - \hat{y}_i) x_{ij}, $$ 

where we sum over all observations *i*, or *stochastic* gradient
descent where we update the $w_j$ by looking at one *i* at a time:

$$ w_j^{t+1} = w_j^t + \lambda (y_i - \hat{y}_i) x_{ij} $$ 

This is the gradient descent method, where we move in the direction
that minimizes the sum of squared error for each predictor.

If we instead use the logistic function for a binary target,

$$ \hat{y} = f(w_1 x_1 + w_2 x_2 + \dots w_p x_p + t), $$

with a logistic function, *f*:

$$ f(z) = \frac{1}{1 + \exp(-z)}, $$

we have the same update rule for gradient ascent on the log likelihood
(here the stochastic version): 

$$ w_j^{t+1} = w_j^t + \lambda (y_i - \hat{y}_i) x_{ij} $$ 

# MNIST handwritten digits

* <http://varianceexplained.org/r/digit-eda/>
* <https://pjreddie.com/media/files/mnist_train.csv>

```{r}
library(readr)
mnist_raw <- read_csv("mnist_train.csv", col_names=FALSE)
dim(mnist_raw)
mnist_raw[1:5,1:5]
library(tidyr)
library(dplyr)
pixels_gathered <- mnist_raw %>%
  head(10000) %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
library(ggplot2)
sub <- pixels_gathered %>% filter(instance <= 12)
ggplot(sub, aes(x, y, fill = value)) + geom_tile() +
  facet_wrap(~ instance + label, labeller=label_both) +
  scale_fill_gradient(low="white", high="black")
```

```{r}
library(caret)
dat <- as.data.frame(mnist_raw[1:10000,-1])/255
y <- factor(mnist_raw$X1[1:10000])
idx <- y %in% c("3","5","8")
dat <- dat[idx,]
y <- droplevels(y[idx])
pc <- prcomp(dat)
plot(pc$sdev[1:20]^2/sum(pc$sdev^2))
x <- pc$x[,1:7]
boxplot(x[,1] ~ y)
ggplot(data.frame(x,y), aes(PC1, PC2, col=y)) + geom_point()
table(y)
tg <- data.frame(k=c(5,9,13,17,21,25))
trCtl <- trainControl(savePredictions=TRUE)
fit <- train(x, y, method="knn", tuneGrid=tg, trControl=trCtl)
ggplot(fit, metric="Kappa")
fit$results
tab <- table(obs=fit$pred$obs, pred=fit$pred$pred)
prop <- tab/rowSums(tab)
round(prop, 3) * 100 # percent
```

```{r}
plotWithSD <- function(fit, param, a=2) {
  min <- with(fit$results, min(Kappa - (a+2)*KappaSD))
  max <- with(fit$results, max(Kappa + (a+2)*KappaSD))
  fit$results$ymax <- with(fit$results, Kappa + a*KappaSD)
  fit$results$ymin <- with(fit$results, Kappa - a*KappaSD)
  ggplot(fit$results, aes_string(param, "Kappa", ymax="ymax", ymin="ymin")) +
    geom_ribbon(fill="black", alpha=.1) +
    geom_point(color="blue") + geom_line(color="blue") +
    ylim(min,max)
}
plotWithSD(fit, "k")
```

```{r}
dat <- as.data.frame(mnist_raw[1:10000,-1])/255
y <- factor(mnist_raw$X1[1:10000])
pc <- prcomp(dat)
x <- pc$x[,1:10]
boxplot(x[,1] ~ y)
ggplot(data.frame(x,y), aes(PC1, PC2, col=y)) + geom_point()
tg <- data.frame(k=c(5,9,13,17,21,25))
trCtl <- trainControl(savePredictions=TRUE)
fit.knn <- train(x, y, method="knn", tuneGrid=tg, trControl=trCtl)
fit.knn$results # 88% Kappa
plotWithSD(fit.knn, "k")
```

```{r}
tab <- table(obs=fit.knn$pred$obs, pred=fit.knn$pred$pred)
prop <- tab/rowSums(tab)
round(prop,3)*100 # percent
diag(prop) <- NA
```

```{r, fig.width=5, fig.height=5.5}
image(prop, xaxt="n", yaxt="n",
      xlab="obs", ylab="pred",
      col=colorRampPalette(c("white","blue"))(50))
for (i in 1:2) axis(i, 0:9/9, 0:9)
abline(0,1,col="red")
mxs <- head(sort(prop,decreasing=TRUE),3)
lbl <- 100*round(mxs,3)
arr.idx <- sapply(mxs, function(i) which(prop == i, arr.ind=TRUE)) - 1
text(arr.idx[1,]/9, arr.idx[2,]/9, labels=lbl, cex=.9, col="white")
```

```{r eval=FALSE}
x <- pc$x[,1:20]
tg <- data.frame(C=c(.5,1,2))
# 700 seconds
system.time({
  fit <- train(x, y, method="svmRadial", trControl=trCtl, tuneGrid=tg)
})
plotWithSD(fit, param="C")
fit$results # 10k data, 20 PCs, 94.5% Kappa
```

```{r eval=FALSE}
x <- pc$x[,1:60]
tg <- data.frame(size=60)
trCtl <- trainControl(method="cv", number=5, savePredictions=TRUE, verboseIter=TRUE)
# 60 s
system.time({
  fit <- train(x, y, method="mlp", trControl=trCtl, tuneGrid=tg, maxit=50)
})
fit$results # 93.8% Kappa
```
