---
title: "optim"
author: "Naim Rashid"
date: "9/3/2018"
output: 
  html_document:
    number_sections: true
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \X \bfm{X}$'
- '$\def \z \bfm{z}$'
- '$\def \thetab \bdm{\theta}$'
- '$\def \betab \bdm{\beta}$'
- '$\def \pib \bdm{\pi}$'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Prior Reading: Givens and Hoeting: Chapter 1

Concurrent Reading: Givens and Hoeting: Chapter 2

A common task in statistics pertains to maximizing (or minimizing) complex univariate or multivariate functions.  This is typically done in the context of the maximization of likelihood functions with respect to a set of unknown parameters, conditional on the observed data.  The term "parameter estimation" may be one that students may have commonly heard in the past during discussions of "fitting" or statistical models to data.  One seeks to find a set of parameter values that "best" fit or explain the data, given a particular likelihood function that is assumed for the problem at hand.  

We will see that our discussion of optimization is a more general form of the special application-specific cases that may have been covered in previous coursework.  How best to optimize such functions depends on the the nature of the function to be optimized as well as practical concerns regarding the complexity of the procedures considered.  

## Example: Height data
Let us assume that we have obtained a random sample of heights (in inches) consisting of 100 individuals, $\mathbf{y} = (y_1,\ldots,y_{100})$, where $y_i$ pertains to the height of the $i$th individual.  Let us further assume that the distribution of these heights is distributed normally with some unknown mean $\mu$ and **known** variance $\sigma^2 = 10$.  That is, we assume that $y_i \sim N(\mu,\sigma^2)$, where $\sigma = 10$.  We can further write these assumptions in the following linear model form akin to previous courses:  $$y_i = \X\beta + \epsilon_i$$ where $\epsilon_i \sim N(0,\sigma^2)$, $\X$ is simply a column of ones, and $\beta = \mu$, a scalar parameter in this setting.  Again,  $\sigma^2$ is known in this case.   Therefore takes the form of a simple linear regression model with just an intercept and no covariates. 

A common question in this situation is what is the best estimate of $\mu$ given the data?  Answering this question can depend on the domain of the question being asked.   In introductory statistical courses, one may remember that an estimator for $\mu$ in this setting may be $\bar{\mathbf{y}}$, or the mean of the observed heights.  In more advanced statistical courses, you may have shown why this is the case via analytically deriving the maximum likelihood estimator for $\mu$ and proving its optimality relative to other potential estimators.  

Similarly, if we recast this problem as a linear regression problem we can show that the MLE for $\mu$ is also $\bar{\mathbf{y}}$. Based on the normal equations from the linear model framework, we have $\hat{\beta} = (\X'\X)^{-1}\X'\y$,  which, plugging in $\X$ and $\y$ from this situation also yields $\bar{\y}$ (can you show this?) if as assume we are using an intercept-only linear model.

Regardingless of the approach used in this particular setting, we can show that we can obtain these estimates analytically where we are optimizing some function of the data.  In the linear regression case, we are minimizing the residual sum of squares of the model with respect to $\beta$ ($\mu$), and in the latter, we are maximizing the likelihood function assumed for the data with respect to $\mu$.  

The likelihood function $L(\thetab)$, given the assumptions we made, can be written as 

\begin{eqnarray}
L(\thetab) &=& f(y_1 | \mu, \sigma^2)f(y_2 | \mu, \sigma^2)\ldots f(y_{100} | \mu, \sigma^2)\\
  &=& \prod_{i=1}^{100} f(y_i | \mu, \sigma^2)\\
  &=& \frac{1}{(2\sigma^{2})^{n/2}}e^{-\frac{1}{2\sigma}\sum_{i=1}^{100}(y_i-\mu)^2}
\end{eqnarray}

Here, $\theta = \mu$ since we are assuming $\sigma^2$ is known.  Taking logs of both sides, we can write the optimization of $L(\thetab)$ in terms of the optimization of $\mathcal{l}(\thetab)$, the log-likelihood, as it is a monotonic transformation of the original likelihood function.  Why does this matter? This implies that the maximum of one function will be the maximum of the other function, as the log transformation is a one-to-one mapping from the likelihood to the log-likelihood (example in figure below).  

Using approaches from prior courses, we can show that this function is smooth and convex in $\mu$, and therefore the maximum of the log-likelihood (and hence the likelihood) will be attained at the value of $\mu$ where the first derivative of $\mathcal{l}(\thetab)$ is equal to zero.  The MLE for $\mu$, $\hat{\mu} = \hat{\y}$ is then obtained  by setting the 1st derivative of the log-likelihood (or likelihood) equal to 0 and then solving for $\mu$ (can you show this?).  In previous courses you may have found that this log transformation often facilitates the taking of derivatives when solving for $\mu$.  This approach of using another function as a proxy of the original function to simplify maximization can be found in several examples of optimation we will cover this semester. 

We can illustrate this approach using the following simulation example:

```{r, echo=TRUE}
# set simulation parameters
mu = 20 
sigma2 = 10
n = 100

# generate random normally distributed samples
y = rnorm(n, mu, sd = sqrt(sigma2))

# define the likelihood function with respect to mu
lik = function(mu, sigma2, y){
 res = rep(NA, length(mu))
 for(i in 1:length(res)) res[i] = prod(dnorm(y, mu[i], sqrt(sigma2))) 
 res
}

# define the log-likelihood function with respect to mu
loglik = function(mu, sigma2, y){
 res = rep(NA, length(mu))
 for(i in 1:length(res)) res[i] = sum(dnorm(y, mu[i], sqrt(sigma2), log = T))
 res
}


# range of mu to plot over
range = seq(0, 40,,1000)

# arrange the two plots in 1 row, two columns
par(mfrow = c(1,2)) 

# likelihood plot
plot(range, lik(mu = range,sigma2 = sigma2, y = y), type = "l", ylab = "likelihood", xlab = "mu")
# vertical line at the mean of y
abline(v = mean(y), lty = 2, col = "grey") 

# log likelihood plot
plot(range, loglik(mu = range,sigma2 = sigma2, y = y), type = "l", ylab = "log likelihood", xlab = "mu")
# vertical line at the mean of y
abline(v = mean(y), lty = 2, col = "grey" ) 

# print MLE of mu, mu_hat
print(mean(y))

# print MLE of mu in linear model framework (intercept only model)
print(lm(y ~1)$coef)
```

We can clearly see that the maximum of both the likelihood and the log-likelihood functions occurs at the MLE of $\mu$, $\hat{\mu}=$ `r round(mean(y),5)`.

In this example, we can derive closed form solutions to obtain a parameter estimate for $\mu$ optimizing the log-liklihood function.  However, what if we do not have such nice closed-form solutions for the model at hand?  We describe several stategies to address this situation in this module.   

## Examples

# Univariate Optimization

In this lecture we will discuss some fundamental methods for the optimization of smooth nonlinear functions (Givens and Hoeting Chapter 2), beginning with single univariate functions and then discussing extensions to the multivariate case.  Combinatorial optimization (Givens and Hoeting Chapter 3) will not be emphasized here but students can read this chapter if interested.  We will also cover the EM algorithm (Givens and Hoeting Chapter 4) in more detail in a later lecture.  There we will discuss the optimization of functions with missing data and also how the algorithm has been widely applied to general problems in statistics.  

Let us consider the numerical optimization of the following function $g(\theta)$, which we seek to maximize with respect to $\theta$: $$g(\theta) = \frac{\log(\theta)}{1+\theta}.$$  The traditional approaches to maximizing $g(\theta)$, such as the approach described in the previous example, are difficult to apply in this case where the analytical solution for $\theta$ after setting $g'(\theta) = 0$  is not easy to determine (try it).  

Lets first plot this function to determine its shape:

```{r, echo=TRUE}
# define function 
f1 = function(x){
  log(x)/(1+x)
}

# make a plot with this function, where the x-axis ranges from 0 to 10
plot(f1, from = 0, to = 10, ylab = "g(theta)", xlab = "theta")
```

Plotting this function, the maximum of $g(\theta)$ appears to be between 3 and 4. So how exactly do we find the maximum of this function, and the value of $\theta$ that pertains to the maximum, $\hat{\theta}$?  One approach is using numerical optimization.

## Basics of Numerical Optimization

Most numerical optimization methods are **iterative** and rely on the concept of successive approximations to find the value of $\theta$ pertaining to the minimum or maximum of the function of interest.  Such algorithms require a **starting value**, an initial guess for $\hat{\theta}$.  

From this point, an algorithm will **iteratively update** its guess for $\hat{\theta}$ based on a predefined updating formula.  Each update may be referred to an "iteration".  

Ideally, the value of $\hat{\theta} \rightarrow \theta^*$ as $t \rightarrow \infty$, where $t$ is the iteration number such that $t>0$ and $\theta^*$ is the "true" value of $\theta$ where the maximum of $g(\theta)$ is achieved.   That is, as the number of iterations increases, we hope that $\hat{\theta}$ will approach its true optimum $\theta^*$, or **converge** to $\theta^*$.

However, given the iterative updating nature of these algorithms, $\hat{\theta}$ will never equal value of $\theta^*$ exactly, and will only approach $\theta^*$ as $t$ increases.  As a result, we need to define criteria or **stopping rules** indicating that the algorithm has **converged**, indicating that $\hat{\theta}$ is reasonably close to the prior value of $\theta^*$ within some **tolerance** or distance.  In some cases, the algorithm may converge to some value other than $\theta^*$, or may not converge at all.  We will discuss these situations later in this lecture.  

### Graphical Illustration

To illustrate an application of numerical optimization to $g(\theta)$ (introduced earlier), see the following plot.  Here, we utilized the commonly used Newton-Raphson algorithm to optimize $g(\theta)$.  In it, we choose a starting value $\theta^{(0)} = 0.25$  to initialize the algorithm.  

```{r chunk-label, fig.show='animate', ffmpeg.format='gif', dev='jpeg', echo = F}
  th = 0.25
    i = h = 0
  plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
  points(th, f1(th), pch = 19, cex = 2)
  abline(v = 3.6)
  text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  for(i in 1:12){
    h = ((th + 1)*(1+1/th - log(th)))/(3+4/th+1/th^2-2*log(th))
     th = th + h
     plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
     points(th, f1(th), pch = 19, cex = 2)
     abline(v = 3.6)
     text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  }

```

You can see that the algorithm appears to **converge** in approximately 12 iterations to `r round(th, 5)` (the value of $\theta$ pertaining to the true maximum of the $g(\theta)$), where the difference between the value of $\hat{\theta}$ at the 11th step ($\hat{\theta}^{(11)}$) and the 12th step ($\hat{\theta}^{(12)}$) is very small, and our final estimate for $\hat{\theta}^{(11)}$ is `r round(th,5)`.  

### Criteria for convergence: absolute versus relative  
If we define our **tolerance** for convergence, or convergence threshold, at step $t$ as some small fixed value $\epsilon$,  we may formally terminate the algorithm when $|\hat{\theta}^{(t)} - \hat{\theta}^{(t-1)}| < \epsilon$.  Here, the true value of $\hat{\theta} = $`r round(th,5)`.  

Clearly, larger values of $\epsilon$ will result in convergence in fewer iterations, however the resulting $\hat{\theta}$ may be less accurate (further aware from $\theta^*$.  Depending on the application, a higher or lower convergence threshold may be warranted, where in some cases 10 decimal places of accuracy (and potentially requiring many more iterations for meeting the convergence threshold) may not be necessary.  

Other criteria for determing convergence exist.  You may notice from the definition above that the algorithm converges if the raw difference in $\theta^{(t+1)}$ and $\theta^{(t)}$ becomes small.  However, if  $\theta^*$  is generally large in scale (say, typically > 10000 in value), an $\epsilon$ of $10^{-6}$ may be too restrictive or difficult to reach, whereas if $\theta^*$ is in the range of $10^{-12}-10^{-16}$, an of $\epsilon$ of $10^{-6}$ may simply be too liberal.  All in all, the threshold's stringency may depend on the scale of $\theta^*$.  

For these reasons some may prefer a relative convergence threshold, such as $|\frac{\theta^{(t+1)} - \theta^{(t)}}{\theta^{(t)}}| < \epsilon$, where now convergence is met if the change relative to $\theta^{(t)}$ is smaller than some proportion $\epsilon$.  There are additional problem-specific variants of this theme, however we will get into this in more detail as we discuss other optimization algorithms. 

### Impact of starting values on convergence
Lets now examine the impact of a different starting point for the algorithm, $\theta^{(0)} = 1$.  

```{r chunk-label2, fig.show='animate', ffmpeg.format='gif', dev='jpeg', echo = F}
  th = 1
    i = h = 0
  plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
  points(th, f1(th), pch = 19, cex = 2)
  abline(v = 3.6)
  text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  for(i in 1:12){
    h = ((th + 1)*(1+1/th - log(th)))/(3+4/th+1/th^2-2*log(th))
     th = th + h
     plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
     points(th, f1(th), pch = 19, cex = 2)
     abline(v = 3.6)
     text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  }
```

Now lets lets try $\theta^{(0)} = 4$.  

```{r chunk-label22, fig.show='animate', ffmpeg.format='gif', dev='jpeg', echo = F}
  th = 4
  i = h = 0
  plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
  points(th, f1(th), pch = 19, cex = 2)
  abline(v = 3.6)
  text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
     
  for(i in 1:12){
    h = ((th + 1)*(1+1/th - log(th)))/(3+4/th+1/th^2-2*log(th))
     th = th + h
     plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
     points(th, f1(th), pch = 19, cex = 2)
     abline(v = 3.6)
     text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  }
```


Now lets try something further away, $\theta^{(0)} = 8$.  

```{r chunk-label3, fig.show='animate', ffmpeg.format='gif', dev='jpeg', echo = F}
  th = 8
    i = h = 0
  plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
  points(th, f1(th), pch = 19, cex = 2)
  abline(v = 3.6)
  text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  for(i in 1:12){
    h = ((th + 1)*(1+1/th - log(th)))/(3+4/th+1/th^2-2*log(th))
     th = th + h
     plot(f1, from = 0, to = 10, ylab = "g(x)", xlab = "theta")
     points(th, f1(th), pch = 19, cex = 2)
     abline(v = 3.6)
     text(x = 7, y = -1.5,labels = sprintf("Iteration %d\n theta_(t) %f \n Difference from prior %f", i, th, h), cex = 1.25)
  }
```

In some cases, the end result of the algorithm is the same regardless of the chosen starting point of the algorithm, however fewer iterations may be required for $\theta^{(t)} \rightarrow \theta^*$ (due to a better starting point).  In other cases, you can see that the model may converge to an  value other than $\theta^*$, representing a **local optimum** rather than the **global optimum**.   Or, the algorithm may not converge at all (diverges), as in the last example example above.  

In general, utilizing a starting value closer to the $\theta^*$ results in convergence in fewer iterations.  This illustrates the importance of choosing an informative starting point for the algorithm, and there are several strategies to do this and can vary by application.  When we examine specific theoretical properties of the optimzation algorithms in the next section, we will see why we may experience convergence vs divergence based on the selected starting point and the shape of the function to be maximized.  

### Basics of Numerical Optimization Summary 
To summarize, the following concepts are important for understanding the set of optimzation algorithms covered in the rest of this module:

1. Algorithm starting point or intialization
2. Choice of updating algoritm
3. Specification of stopping criteria

In the next few sections we will go into detail regarding various approaches for numerical optimazation and their pros and cons for use. We will also discuss algorithm-specific aspects of computational complexity and convergence speed of each approach to help you decide which algorithm may be better to utilize for your specifical application. 


## Newton-Raphson
In the previous illustration we used a common technique called **Newton-Raphson** (NR) to optimize the function of interest with respect to $\theta$. This approach is also referred to as Newton's Method, and is an iterative updating scheme to arrive at $\hat{\theta}$.  

One notable requirement of this method is that expressions for  the **first and second derivatives of the function of interest must be given**.  In some situations such derivatives may be difficult to derive or evaluate in practice. We will introduce several alternative numerical optimization procedures to handle these situations later in this lecture. 

Let $\theta^{(0)}$ denote the starting value for the estimate of $\theta$ and assume that we wish to maximize some function $f(\theta)$.  Then, based on this scheme, we update our estimate of $\theta$ at the $t$th step of the algorithm ($\theta^{(t)}$) using the following expression: $$\theta^{(t+1)} = \theta^{(t)} + h^{(t)},$$ where $h^{(t)} = -\frac{f'(\theta^{(t)})}{f''(\theta^{(t)})},$ $f'(\theta)$ is the first derivative of $f(\theta)$ with respect to $\theta$, and $f''(\theta)$ is the second derivative of $f(\theta)$ with respect to $\theta$.

If we sought to instead minimize $f(\theta)$, we would instead choose $h^{(t)} = \frac{f'(\theta^{(t)})}{f''(\theta^{(t)})}$ (this is equivalent to maximizing $-f(\theta)$).   After choosing an appropriate starting point for the algorithm and stopping rule, we can apply the updating rule above as we had in the previous example to arrive at $\hat{\theta}$.  

A few obvious questions:

1.  How did we get this expression?
2.  Why would we even expect $\theta^{(t)} \rightarrow \theta^*$?
3.  Under what conditions do we expect the algorithm to work well or not work well?
4.  How quickly would we expect $\theta^{(t)} \rightarrow \theta^*$ in $t$?

There may be situations where the algorithm may not converge or do so in a suitable timeframe.  Understanding the rationale behind this approach and its properties may help navigate these situation may provide clues to what alternative approaches may work. 

### Rationale and Derivation
Let us suppose that $g'(\theta)$ is continually differentiable, $g''(\theta)$ exists, and that $g''(\theta) \neq 0$ (suggesting that the derivative is non-constant).  At a given iteration $t$, $g'(\theta^*)$ can be approximated by a linear Taylor Series Expansion about $\theta^*$, the unknown true value of $\theta$ that maximized $f(\theta)$.  Yes, we do not know what $\theta^*$ is in reality, but this term is important when we talk about the conditions that may impact the performance of this approach later on. 

Under this approximation, we have $$0 = g'(\theta^*)  \approx g'(\theta^*) + (\theta^* - \theta^{(t)})g''(\theta^{(t)}).$$  Lets unpack what this means for one second.  If you recall, most analytic approaches to optimizing algorithms start with setting the 1st derivative to 0 and solving for the value of $\theta$ satisfying this equality.  In this setting, we assume that such an analytic solution does not exist, and therefore we utilize numerical optimization approaches to arrive at an approximate solution.  In addition, we can see that we that in using this linear Taylor Series approximation, g'(\theta^*) is approximated by its tangent line at $\theta^{(t)}$ (see figure below).  
 
![Example of Newton Raphson in the context of the previous illustration.  The algorithm approximates $g'$ by its tangent line at $\theta^{(t)}$, whose root $\theta^{(t+1)}$ serves as the next approximation of the true root $\theta^{*}$.](chap2figgoodnewt.jpg){width=65%}

Since we are approximating $g'(\theta^*)$ with  its tangent line at $\theta^{(t)}$, it makes sense that we can approximate the root of $g'(\theta^*)$ (the point at which it equals 0) with the root of the tangent line at  $\theta^{(t)}$ (see example in above figure at step 1).  Under this rationale, we solve for $\theta^{(*)}$ in the linear Taylor Series approximation above.  Doing this, we have $$\theta^*  = \theta^{(t)} - \frac{g'(\theta^{(t)})}{g''(\theta^{(t)})} = \theta^{(t)} + h^{(t)}.$$  In other words, the value for $\theta^*$ is approximated using the current guess $\theta^{(t)}$ and a refinement step $h^{(t)}$, and sucessive iterations of this approach will yield closer and closer approximations to $\theta^*$.  

### Updating Equation
Using the above rationale, we can define the updating strategy as  $$\theta^{(t+1)}  = \theta^{(t)} - \frac{g'(\theta^{(t)})}{g''(\theta^{(t)})} = \theta^{(t)} + h^{(t)}.$$  

When the optimization of $g$ corresponds to maximum likelihood estimation, where $\hat{\theta}$ is the solution to $l'(\theta) = 0$, the updating equation is given as $$\theta^{(t+1)}  = \theta^{(t)} - \frac{l'(\theta^{(t)})}{l''(\theta^{(t)})}.$$  


### Requirements and Convergence
But how do we know that successive iterations using the updating equation above will results in convergence to $\theta^*$, rather than diverging or converging to some other value?  In practice, this depends on the shape of the function you are maximizing as well as your chosen starting value.  As we can see from the prior examples, choosing starting values too far away from $\theta^*$ results in slower convergence or even divergence.  Choosing values closer to the $\theta^*$ results in quicker convergence.  

We can show that if $g'''(\theta)$ is continuous and that $\theta^*$ is a simple root of $g'(\theta)$ (implying $g'(\theta^*) = 0$), then there exists a *neighborhood* around $\theta^*$ for which NR converges to $\theta^*$ from any starting value $\theta^{(0)}$ within that neighborhood (GH 2.1.1, eq. 2.18).  The definition of "neighborhood" in this sense is somewhat arbitrary, but is meant to indicate that convergence is likely if you are in some close vicinity of $\theta^*$.  GH provides this proof using arguments based upon the error of a quadratic Taylor Series approximation for $g'(\theta)$ in Section 2.1.1. 

More specifically, if $g'(\theta)$ is twice differentiable, is convex, and has a root, then NR will converge from any point!  These conditions are typically met by common likelihood functions in statistics (but not always).

If you are starting in some arbitrary interval $[a,b]$, you can check the following conditions to determine whether the NR algorithm will converge from any $\theta^{(0)}$ in that interval:

1.  $g''(\theta) \neq 0$ on $[a,b]$
2.  $g'''(\theta)$ does not change sign on $[a,b]$
3.  $g'(a)g'(b) < 0$, and
4.  $|g'(a)/g''(a)| < b-a$ and $|g'(b)/g''(b)| < b-a$

These statements follow from the proof given in GH 2.1.1.  In practice you may not need to do this before applying NR, but in cases were problems arise, can help explain problems relating to convergence if observed.  Rephrased, we can translate these conditions into the following:

NOTE, check if these translations are correct

1.  The 2nd derivative is never equal to zero in the interval $\rightarrow$ $g'(\theta)$ is never flat on $[a,b]$ $\rightarrow$ $g'(\theta)$ can potentially to intersect 0 in the interval $\rightarrow$ $g'(\theta)$ has the potential have a root in $[a,b]$ 
2.  The third derivative does not change sign on $[a,b]$ $\rightarrow$ the second derivative stays convex in the interval on $[a,b]$ 
3.  Implies that the 1st derivative at $a$ is positive and negative at $b$ (or vice versa) $\rightarrow$ $g'(\theta)$ changes sign at some point in $[a,b]$ $\rightarrow$ implies a root for $g'(\theta)$ must exist in $[a,b]$
4.  The absolute value of $h$ at $a$ or at $b$ is less than the length of the interval $\rightarrow$ the step size (refinement) assuming $\theta^{(0)} = a$ or $\theta^{(0)} = b$ is smaller than the length of the interval being considered.  If this is not true, the update will definitely not be contained in $[a,b]$. 

Using similar arguments, we can show that the convergence order for NR is *quadratic*, meaning the accuracy of the solution will double with each iteration $t$.   Higher convergence order implies that accurate approximations for the parameter(s) of interest are achieved in fewer iterations.  However, we will see that algorithms with higher convergence orders may also be less robust to different conditions and may fail more frequently than slower algorithms.  We will show some examples of these in later lectures. 


### Pros and Cons
* Pros
    + Speed:  Extremely Fast
* Cons
    + Requires derivation and evaluation of 1st and 2nd derivatives
  
If the derivatives and/or likelihood are complicated NR may not be an attractive alternative.  In addition, if the derivatives required additional work for evaluation (such as likelihoods involving integration), perform additional evaluations in the 1st and 2nd derivatives may be unattractive. 


## Fisher Scoring
Fisher Scoring is an alternative to NR when performing maximum likelihood estimation, where we simply replace $l''(\theta^{(t)})$ with $I(\theta^{(t)})$, the expected Fisher Information matrix at iteration $t$.  We can show that $l''(\theta)$ is simply an approximation for $I(\theta)$ (GH 1.4), so it is not surprising that the asymptopic properties for NR and FS are similar. 

The updating equation is given as $$\theta^{(t+1)}  = \theta^{(t)} - l'(\theta^{(t)})I(\theta^{(t)})^{-1}.$$ Given the particular problem at hand, NR or FS may be easier to derive analytically, where the latter only needs knowledge of the 1st derivatives and avoids computation of the 2nd derivatives. According to GH, FS may be used in early iterations for rapid improvements, and NR can be used to make better refinements near the end.  

* Pros
    + Avoids derivation and computation of the 2nd derivative
    + Similar asymptotic properties of NR
* Cons
    + May not be as good for refinement near end (CHECK THIS)

## Secant Method
This approach is similar to NR, except that the second derivative in the updating equation is replaced with a finite-difference approximation, where now the updating equation is $$\theta^{(t+1)}  = \theta^{(t)} - g'(\theta^{(t)})\frac{\theta^{(t)} - \theta^{(t-1)}}{g'(\theta^{(t)}) - g'(\theta^{(t-1)})}.$$ Conditions for convergence are similar to NR, but we can show through similar arguments that that the convergence order for the secant method is 1.62 instead of 2.

* Pros
    + Avoids derivation and computation of the 2nd derivative
    + Similar asymptotic properties of NR
* Cons
  + Slower convergene relative to NR (what about FS?)

## Examples

# Multivariate Optimization

The approaches that we have described also extend to the multivariate setting, where we seek to optimize a function with respect to more than one parameter at the same time.  More formally, we see to find the optimum of some real valued function $g(\thetab)$, where $\thetab$ is now some $p-$dimensional vector of parameters such that $\thetab = (\theta_1,\ldots,\theta_p)^T$.  Similar to before, the estimate of $\thetab$ at step $t$ will be denoted as $\thetab^{(t)}$.  Most of the prior concepts covered in univariate maximimization, such as regarding starting points, iterative updating, and specification of convergence criteria for termination, also applies here.  

## Note about convergence in the multivariate setting 

Obviously in the multivariate setting we cannot directly utilize the same convergence criteria defined earlier.  To extend this to the multivariate setting, we may use distance-based measures for convergence, for example based one the sum of the absolute differences between iterations $$D(\mathbf{u},\mathbf{v}) = \sum_{i-1}^p | u_i - v_i|$$ or the euclidean distance between iterations $$D(\mathbf{u},\mathbf{v}) = \sum_{i-1}^p (u_i - v_i)^2$$.  The output of these functions are scalars, so given a particular choice of $D(\mathbf{u},\mathbf{v})$, we can define a particular absolute convergence threshold $\epsilon$ to terminate the algorithm such that $D(\thetab^{(t+1)},\thetab^{(t)}) < \epsilon$.  We can similarly define a relative convergence threshold $\epsilon$ such that $$\frac{D(\thetab^{(t+1)},\thetab^{(t)})}{D(\thetab^{(t)},0)} < \epsilon$$.  

We will now discuss multivariate extensions to the methods introduced in the previous section.  

##  Newton-Raphson and Fisher Scoring

Using the rationale described in the univariate setting, we can define the updating algorithm in this setting using a quadradtic Taylor Series expansion around $g(\thetab^*)$:  $$g(\thetab^*) = g(\thetab^{(t)}) + (\thetab^* - \thetab^{(t)})^T g'(\theta^{(t)}) + \frac{1}{2}(\thetab^* - \thetab^{(t)})^T g''(\theta^{(t)})(\thetab^* - \thetab^{(t)}).$$ If we take the derivative of this expansion and set it equal to zero, we get $$0 = g'(\thetab^{(t)}) + g''(\thetab^{(t)})(\thetab^* - \thetab^{(t)}),$$  which provides the update $$\thetab^{(t+1)} = \thetab^{(t)} - g''(\thetab^{(t)})^{-1}g'(\thetab^{(t)})$$ As a side note, if you recall, in our derivation from the univariate setting we started with a linear Taylor Series expansion of $g'(\theta^*)$.  Doing the same in this setting, rather than starting with a quadratic Taylor Series expansion around $g(\thetab^*)$ would allow us to arrive at the same expression given above.  So, our increment $\mathbf{h}^{(t)}$ is simply  $\mathbf{h}^{(t)} = -g''(\thetab^{(t)})^{-1}g'(\thetab^{(t)})$.  Fisher scoring in this case also has a similar updating function $$\thetab^{(t+1)} = \thetab^{(t)} - \I(\thetab^{(t)})^{-1}\l'(\thetab^{(t)})$$.  The properties of these algorithms will be similar to those described in the univariate section.  

### Example

## Iteratively Reweighted Least Squares
Generalized Linear Models (GLMs) encompass a large family of models including linear regression, logistic regression (binary responses), poisson regression (count responses), and many others.  In such models, the response variable $Y_i$, $i=1,\ldots,n$ are independently distributed with some distribution parameterized by $\theta_i$.  In contrast to linear regression, the class of models encompassed by GLMs can handle response variables of different distributions, where such distributions are members of the exponential family.  Similar to linear regression, we are often attempting to maximize a given likelihood function with respect to multiple regression coefficients and other parameters. 

This family has the general form $$ f(i|\theta) = exp{[y\theta-b(\theta)]/a(\phi) + c(y,\phi$)},$$ where $\theta$ is called the "natural" or "canonical" parameter and $\phi$ is the dispersion parameter.  The following distributions can be factored into this form, and therefore belong to the exponential family:

* Bernoulli
* Poisson
* Normal
* Exponential
* Gamma
* Chi-Squared
* Beta
* Dirichlet
* Wishart
* Geometic
* Binomial (fixed number of trials)
* Negative Binomial (fixed number of failures or fixed overdispersion parameter)
* Multinomial (fixed number of trials)

Note that the last three distributions have conditional membership to this family of distributions, which in some cases impacts how they are maximized (an example of this will be given later). 

For these distributions, we  can factor their PDFs/PMFs into the form above to determine $b(\theta)$, $a(\phi)$, and $c(y,\phi)$.  We can then utilize these quantities to estimate $\theta$ and $\phi$ in a general unified framework.  For example, we can show that $E[Y] = b'(\theta)$ and $var[Y] = b''(\theta)a(\phi)$.  

Similar to linear regression, we often wish to model $Y_i$ with respect to some set of covariates $\x_i$.  In this particular model, we assume that the relationship between $E[Y_i |\x_i]$ and $\x_i$ can be modeled such that $g(E[Y_i |\x_i]) = \x_i\betab$, where $\betab$ is a vector of unknown parameters to be estimated and $g$ is called the "link function".  For example, in linear regression $g$ is simply the "identity link"" where $E[Y_i |\x_i] = \x_i\betab$ and no transformation is performed.  For each member of the exponential family, different link functions may be utilized, although some may be more mathematically convenient to use than others.  You will learn more about these models and various link functions in Bios 663 and Bios 762.  

Given the wide range of distributions that may be covered by the exponential family, a general estimation framework to obtain $\hat{\betab}$ may be advantageous for modeling, as separate algorithms for different distributions can be avoided.  If we take the example of logistic regression, we assume that the response variable $Y_i$ is distributed Bernoulli with parameter $\pi_i$, where $E[Y_i |\x_i] = \pi_i$, where $i =  1,\ldots,n$.  We assume here that the probability of success for individual $i$ is dependent on an $n \times p$ matrix of predictors $\X$ and a set of unknown regression coefficients $\betab$, a $p\times 1$ vector.  Since this distribution is part of the exponential family, we can factor the PDF and show that the natural parameter is $\theta_i = log(\frac{\pi_i}{1-\pi_i}), a(\phi) = 1$, and $b(\theta_i) = log(1+exp(\theta_i))$.  

The natural or canonical link function in this case is the logit function, where $g(E[Y_i |\x_i]) = g(\pi_i) = log(\frac{\pi_i}{1=\pi_i}) = \X\betab$, which is has a form similar to that of $b(\theta_i)$ (hence the name "natural" or "canonical").  Rearranging terms, this link function implies that $\pi_i = \frac{exp( \X\betab)}{1+ exp( \X\betab)}$.  We can then show that the likelihood for this model can be written in the form $$l(\betab) = \y^T\X\betab - \mathbf{b}^T\mathbf{1},$$ where $\y = (y_1,\ldots,y_n),$ $b = (b(\theta_1),\ldots,b(\theta_n)$.  Given we use the cannonical line, we simply replace $\theta_i$ in the likelihood with $\X\betab$, which in certain cases can simplify the derivation.  

## NR-based approach to maximization
Using similar logic as before to maximize the likelihood with respect to $\betab$, let us try to find the value of $\betab$ such that the first derivative of the likelihood function is equal to zero.  Carrying out this derviative, we can show that the first derivative of the likelihood function (sometimes called the score function) can be written as $$\l'(\betab) = \X^T(\y - \pib),$$ where $\pib = (\pi_1,\ldots,\pi_n)$.  Notice that this expression is now a $p$-dimensional vector, as we are taking the derivative of the likelihood with repect to each element of $\betab$, which is $p-dimensional$.  The second derivative of the likelihood function (sometimes called the "Hessian") is given as $$\l''(\betab) = \frac{d}{d\betab}(\X^T(\y-\pib)) = -\left(\frac{d\pib}{d\betab}\right)^T\X = -\X^T\W\X,$$ where $\W$ is a diagonal matrix with the $i$th diagonal element having value equal to $\pi_i(1-\pi_i)$.  Note that the Hessian is a matrix.  

Plugging in these values into the NR algorithm defined earlier, the updating algorithm is given as $$\beta^{(t+1)} = \beta^{(t)} - \l''(\betab)^{-1}l'(\beta^{(t+1)}) =  \beta^{(t)} + (\X^T\W^{(t)}\X)^{-1}\X^T(\y - \pib^{(t)}),$$ where $\pib^{(t)}$ depends on $\betab^{(t)}$ and $\W$ is a function of $\pib^{(t)}$.  Given the fact that the Hessian does not depend on the observed data, we can see that the Fisher's Information Matrix is equal to the Observed information matrix, where $\I(\betab) = E[-\l''(\betab)]  = E[\X^T\W\X] = -l''(\betab)$.  

As a result, FS and NR are exactly the same when the link function is chosen as the canonical/natural link.  In other words, when the link function is similar in form as the natural parameter $\theta_i$. 

## FS and IRLS-based approach to maximization for arbitrary link functions

Under non-canonical links, the derivatives of the likelihood function become much more complicated, and methods such as FS may be more amenable to implement in this scenario.

More generally, we can show that FS (for both canonical and non-canonical links) in this setting is similar to performing weighted least squares, where we can defined an "error" or "residual" vector $$\e^{(t)} = \y - \pib^{(t)}$$ and a "working response" vector $$\z^{(t)} = \X\betab^{(t)} + (\W^{(t)})^{-1}\e^{(t)}.$$  Plugging in $\e$ into the FS update written above and then rearranging and factoring terms, we have $$\betab^{(t+1)} = (\X^T\W^{(t)}\X)^{-1}\X^T\W^{(t)}z^{(t)}.$$  Looking to results from Bios 663, it is clear that this expression has the form similar to the estimate for $\hat{\betab}$ in the standard linear model ($(\X^T\X)^{-1}\X^ty)$, except instead of $\y$ we have the working response vector $\z$ and a weight matrix $\W$ involved.  This diagonal weight matrix is often found in weighted linear regression, where such weights are used to correct for non-constant variance between observations (heteroscedasticity). In any case, in each iteration we update the estimate of $\betab^{(t+1)}$ given the current values of $\e^{(t)}$ and $\W^{(t)}$, then then in turn update these intermediate quantities given $\betab^{(t+1)}$, continuing until convergence.  This update can be computed "manually" using matrix computation, or using existing off the shelf software/algorithms for weighted least squares.  

We call this iterative approach of applying weighted least squares as Iteratively Reweighted Least Squares (IRLS). 

The ease in implementation of this approach is one reason why we see IRLS as the standard approach for maximimizing GLMs in R and SAS.  Its connection to weighted least squares is another reason why it is often used in tandem with penalized likelihood approaches for high dimensional variable selection, as one can apply the same procedures used in linear models to GLMs given the framework above. 

In summary, we have the following pro's and cons for using IRLS

* Pros [update with additional]
    + Single implementation for wide range of models belonging to the exponential family (update form of $\W$ and $\e$ and given distribution)
    + Can handle arbitrary link functions
    + Easy to extend certain methods developed for the linear model to the GLM setting (diagnostics, penalization, etc)
* Cons
  +  Can be slow and unreliable, unless the data fits model well CHECK THIS
  
## Example:  Poisson and Negative Binomial regression with IRLS


## Newton-like methods

A wide variety of "Newton-Like" methods exists, where the updating equation takes on a form similar to NR but usually approximates the matrix of 2nd derivatives (Hessian) with a simpler approximation.  Here, expressions for only the **first derivatives of the function of interest are needed** and expressionsions for the **second derivatives are not needed**.  

This is helpful when the second derivatives may be difficult to derive, have a complicated form, or are computationally expensive to evaluate in practice. 

For example, in the class of methods we may write the updating equation as $$\thetab^{(t+1)} = \thetab^{(t)} - (\M^{(t)})^{-1}\g'(\thetab),$$ where $\M^{(t)}$ is q $p\times p$ matrix approximating the Hessian $\g''(\thetab)$ (assuming $\thetab$ is a $p$-dimensional vector).  We may choose to do this because either evaluating the Hessian is too computationally expensive, or the steps selected by NR may not go uphill, as it is not guarunteed that $g(\thetab^{(t+1)}) > g(\thetab^{(t)})$ when we are maximizing some function $g(\thetab)$.  

Alternatively, we can choose an $\M^{(t)}$ that can guarantee $g(\thetab^{(t+1)}) > g(\thetab^{(t)})$ (ascent).  In FS, we are essentially doing this by replacing the Hessian with $M^{(t)} = -\I(\thetab^{(t)})$.  

Due to time purposes, we will not cover Ascent Algorithms, Discrete Newton, Fixed Point Methods or Gauss-Newton in detail.  However, if interested you can find these topics covered in GH 2.2.2.   We will instead cover more commonly used algorithms such as BFGS (which in general has better performed from the aforementioned methods) and Nelder Mead in this lecture.  

In general, one may try to avoid calculation of the Hessian matrix by approximating it using an approach akin to the secant-absed finite-difference approaches detailed in the univariate maximization portion of this lecture (discrete Newton or fixed point methods do this).  However, this can become computationally burdensome especially if M is of larger dimension and because of the fact that $M^{(t)}$ may have to be updated at each iteration to ensure faster convergence.  

### Quasi-Newton Methods:  BFGS

Instead, we can use "Quasi-Newton" methods where $M^{(t)}$ is updated with knowledge of the curvature of $\g$ in the direction of the proposed step $h^{(t)}$ near $\thetab^{(t)}$ while we are performing the update $\thetab^{(t+1)} = \thetab^{(t)} + h^{(t)}$.  We would like to to avoid computing the approximation of each element the Hessian matrix one-by-one to reduce computational burden.  At the same time, we would also like to retain a similar secant-type condition where $$\g'(\theta^{(t+1)}) - \g'(\thetab^{(t)}) = M^{(t+1)}(\thetab^{(t+1)} - \thetab^{(t)}),$$ essentially approximating the finite-difference approach using $M^{(t+1)}$.  This equation implies that the specification of some Matrix $\M^{(t+1)}$ times the difference in the estimate of $\thetab$ between iterations is equal to the finite difference approximation, preserving the secant condition.  THIS NEEDS TO BE BETTER ELABORATED ON. 

The question is how do we obtain an $\M^{(t+1)}$ that satisfies this condition and is computable in a manner that is efficient?  Quasi-Newton (or "variable step") methods are one such approach that can do this when approximating the Hessian at each iteration  The update to the Hessian has the following form $$\M^{(t+1)} = \M^{(t)} - \frac{\M^{(t)}\z^{(t)}(\M^{(t)}\z^{(t)})^T}{(\z^{(t)})^T\M^{(t)}\z^{(t)}} + \frac{\y^{(t)}(\y^{(t)})^T}{(\z^{(t)})^T\y^{(t)}} + \delta^{(t)}(\z^{(t)})^T\M^{(t)}\z^{(t)})\d^{(t)}(\d^{(t)})^T,$$ where $$\d^{(t)} = \frac{\y^{(t)}}{(\z^{(t)})^T\y^{(t)}} - \frac{\M^{(t)}\z^{(t)}}{(\z^{(t)})^T\M^{(t)}\z^{(t)}}.$$  

This class of algorithms is indexed by the value of $\delta^{(t)}$, where $\delta^{(t)} = 0$ represents the populat BFGS update.  BFGS is generally regarded as the best performing method in this class and is commonly used.  We wont worry about the specific details regarding how this updated was specifically derived.  In addition, many off-the-shelf methods are available implementing this procedure, given a known likelihood function and 1st derivative (we will give an example of this later), so no manual implementation of this appraoch is necessary. 

### Tips for improving performance and stability

Performance of BFGS is senstive to the starting value of $\M^{(0)}$.  For maximum likelihood estimation, it is good to set $\M^{(0)} = -I(\thetab^{(0)})$.  For other problems, setting $\M^{(0)} = -I_{p\times p}$, the negative identity matrix, may work well if all the parameters are on similar scales.  

Rescaling parameters is generally helpful if parameters are on very different scales, for example if you have a function $g(\theta_1, \theta_2) = exp(1+\theta_1) + \theta_2$.  Clearly, similar changes in $\theta_1$ and $\theta_2$ will have very different impacts on $g(\theta_1, \theta_2)$.  This may also prevent the stopping criterion from being influenced by those variables that have the largest units or most influence on the function to be maximized over.   We will go more in how to do this in the section on $\texttt{optim} in R.


NEED TO ADD SOME TEXT ON BACKTRACKING


### Pros and Cons for using BFGS:

* Pros
    + Avoids calculation of the Hessian in an efficient manner, while retaining the secant condition
    + Fast and powerful
    + Backtracking approach can ensure asecent
* Cons
    + Not as fast as NR (convergence order between 1 and 2), faster than linear but slower than quadratic convergence
    + Complicated to implement manually, so reliance on off the shelf methods are needed. 

## Nelder-Mead

In certain situations we may find that neither the first nor the second derivative may be easily derived or easy to evaluate computationally.  Nelder-Mead , sometimes called the "Simplex Search Method" is one such algorithm that only requires the specification and evaluation of the function in question $g(\thetab)$ in order to perform optimization. Hence this algorithm falls into the class of iterative "direct search" algorithms as it only depends on the ranks of function search evaluations at possible solutions as it attempts to nominate a better point for the algorithm to move to in the parameter space. 

In the maximum likelihood estimation context, this means that only the likelihood function is evaluated during optimization.  This method may also be applied in more general situations where only some objective function exists that you wish to maximize or minimize but not clear analytical derivatives exists. 

Prior descriptions of this algorithm describe it as "amoeba-like", where the algorithm sequentially updates the "best" set of model parameters, crawling uphill or downhill in terms of the objective function surface until it reaches and optimal set of parameters.  The specific algorithm and its development are somewhat out of the scope of this course, however we will cover it briefly here.  

The main ideas behind this approach is similar to the prior two classes of algorithms covered so far, where the algorithm starts from an initial set of guesses for the parameter estimates, and then iteratively updates until some convergence criteria is met.  

### Updating algorithm
The basic idea behind this algorithm is based upon the definition of the "simplex", which is essentially a $p +1$ dimensional space based upon the $p$ parameters of a given model.  Each of the $p+1$ elements of the simplex contains a $p$ dimensionals vector representing a particular combination of the $p$ parameter values.  Each one of these $p$ combinations represent a "vertex" in the simplex space.  The method begins from set of initial vertices in the simplex based upon $\thetab^{(0)} through some predefined algorithm.  The set of vertices is then then modified one by one, keeping modifications that leads to an improvement in the objective function.  

In this iterative fashion, the vertices in the simplex slowly start to move towards regions of higher value until no further updates to the parameters results in an appreciable increase in the objective function.  The simplex may also "shrink" in size to better encapsulate regions of higher values when proposed moves in the simplex do not results in significant improvements in the objective function.  In general, the algorithm in each iteration tries to improve the worst point in the simplex (corresponding to a single parameter in the original model). 

The details of this updating procedure is given in GH, however we illustrate this graphically in the video below pertaining to the optimization of a 2D model (containing two parameters).

<iframe width="420" height="315" src="https://www.youtube.com/embed/HUqLxHfxWqU"></iframe>


### Convergence

Relative or absolute convergence criteria may be based upon the objective function values at each iteration of the algorithm or based on the volume of the simplex at the given set of vertices.  

### Pros and Cons
* Pros
    + Does not require 1st or 2nd derivatives
    + Robust:  can be applied to a wide range of algorithms
    + Efficient implementation typically is helpful when evaluation of the objective funciton may be computationally expensive
* Cons
    + Typically requires many more iterations for convergence, and therefore may not be optimal for large-scale testing problems. 
    + Difficult to implement manually but many off-the-shelf solutions exist (such as optim in R)
    +  Not suitable for high dimensional problems
  
## Examples 







