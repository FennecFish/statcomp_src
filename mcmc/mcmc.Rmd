---
title: "MCMC"
author: "Naim Rashid"
date: "2/7/2019"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bftm}[1]{\ensuremath{\mathbf{#1$^{T}$}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \A \bfm{A}$'
- '$\def \b \bfm{b}$'
- '$\def \tA \bftm{A}$'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \Y \bfm{Y}$'
- '$\def \X \bfm{X}$'
- '$\def \x \bfm{x}$'
- '$\def \tx \bftm{x}$'
- '$\def \z \bfm{z}$'
- '$\def \betab \bdm{\beta}$'
- '$\def \Omegab \bdm{\Omega}$'
- '$\def \pib \bdm{\pi}$'
- '$\def \thetab \bdm{\theta}$'
- '$\def \epsilonb  \bdm{\epsilon}$'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
In the previous lecture we discussed Monte Carlo Integration (MCI), where we drew samples from some target distribution $f$ in order to approximate the integral $\int h(\x)f(\x)d\x$.  We also discussed ways to sample from an alternative density $g$ to reduce the variance of the standard MC estimator via importance sampling.  This could be applied even when the target density $f$ cannot be sampled from and is only known up to a multiplicative proportionality constant. Importance sampling can bring enormous gains with a good choice of $g$ by oversampling the "important" regions of the integrand, making an otherwise infeasible problem amenable to MCI. It can also backfire with a bad choice of $g$ , yielding an estimate
with infinite variance when simple Monte Carlo would have had a finite variance.

In our discussion of importance sampling, we are focused on integration.  But what if need to directly obtain samples from a target distribution $f$ when no software/method is available to do so?  This is common in bayesian applications where sampling from some posterior distribution is required but is not easy to do (the exception is when conjugate priors are used).  The ability to sample exactly (or approximately) from $f$ can also be helpful for applying MCI. We will see later in this lecture that Markov Chain Monte Carlso (MCMC) approaches in particular may be preferable in cases where the dimension of the integral starts to become very large. We will discuss the relative pro's and cons of each approach introduced below, motivated by our Poisson GLMM example from the previous lecture. 

# Motivating Example:  MCEM-based estimation of the Poisson GLMM example

Lets assume that we wish to utlize MCEM to fit the Poisson GLMM from the previous lecture.  If we recall, the likelihood can be written as the following:

$$L(\boldsymbol{\beta}, \sigma^2_{\gamma} | \boldsymbol{y}) = \prod_{i = 1}^{22}\int \left[f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i})\phi(\gamma_i| 0, \sigma^2_{\gamma}) \right] d\gamma_i$$

where $f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) = \prod_{j = 1}^5 f(y_{ij} | \lambda_{ij})$, $f(y_{ij} | \lambda_{ij})$ is the Poisson PMF with mean $\lambda_{ij}$ for the $i$th patient in the $j$th month, $\gamma_i$ is the unobserved subject-level random effect, $\log(\lambda_{ij}) = \boldsymbol{x}_{ij}\boldsymbol{\beta} + \gamma_i$, and $\phi(\gamma_i| 0, \sigma^2_{\gamma}) \sim N(0,\sigma^2_{\gamma})$.  Here  $\boldsymbol{x}_{ij} = (1, j)$ with correponding $\boldsymbol{\beta} = (\beta_0,\beta_1)^T$, where $\beta_0$ is the population intercept $\beta_1$ is the regression coefficient for month.  Also,  $\boldsymbol{y} = (\boldsymbol{y}_1^T,\ldots,\boldsymbol{y}_n^T)^T$ is the vector of all observations in the study,  $\boldsymbol{y}_i$ is $5\times 1$ vector of words recalled in each month for subject $i$, $\boldsymbol{\lambda}_i = (\lambda_{i1},\ldots, \lambda_{i5})$,and $\boldsymbol{\gamma} = (\gamma_1,\ldots,\gamma_{22})$.

If we assume $\boldsymbol{\gamma}$ was known, then we could write the CDLL as 

\begin{aligned}
\log L_c(\boldsymbol{\beta}, \sigma^2_{\gamma} | \boldsymbol{y}, \boldsymbol{\gamma}) &= \log\left(\prod_{i = 1}^{22} f(\boldsymbol{y_{i}} | \boldsymbol{\lambda}_{i})\phi(\gamma_i| 0, \sigma^2_{\gamma})\right)\\
&=\sum_{i = 1}^{22} \log f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) + \log \phi(\gamma_i| 0, \sigma^2_{\gamma})\\
\end{aligned}

where $\log f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) = \sum_{j = 1}^{5}\log f(y_{ij} | \lambda_{ij})$.  Then, the Q-function at step $t$ can we written as the following:

\begin{aligned}
Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) &= E[\log L_c(\boldsymbol{\beta}, \sigma^2_{\gamma} | \y, \boldsymbol{\gamma}) | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}]\\
&=\sum_{i = 1}^{22} \int \left[\log f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) + \log \phi(\gamma_i| 0, \sigma^2_{\gamma})\right]f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})d\gamma_i
\end{aligned}

where $f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})$ is the conditional expectation for $\gamma_i$ given the observed data and current parameter estimates.  This density has the following form

\begin{aligned}
f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) &= \frac{f(\gamma_i, \boldsymbol{y}_o| \boldsymbol{\theta}^{(t)})}{f( \boldsymbol{y}_o| \boldsymbol{\theta}^{(t)})}\\
&=\frac{f(\boldsymbol{y}_o| \gamma_i ,\boldsymbol{\theta}^{(t)})f(\gamma_i |\boldsymbol{\theta}^{(t)})}{\int f(\boldsymbol{y}_o| \gamma_i ,\boldsymbol{\theta}^{(t)})f(\gamma_i |\boldsymbol{\theta}^{(t)}) d\gamma_i}\\
&=\frac{f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{(t)})\phi(\gamma_i| 0, (\sigma^2_{\gamma})^{(t)})}{\int f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{(t)})\phi(\gamma_i| 0, (\sigma^2_{\gamma})^{(t)}) d\gamma_i}\\
\end{aligned}

where $\boldsymbol{\lambda}_i^{(t)} = (\lambda_{i1}^{(t)},\ldots, \lambda_{i5}^{(t)})$ abd $\log(\lambda_{ij}^{(t)}) = \boldsymbol{x}_{ij}^T\boldsymbol{\beta}^{(t)} + \gamma_i$.  For a given subject $i$, we are evaluating a one-dimensional integral with respect to $\gamma_i$ to evaluate the Q-function.  Therefore, methods from the prior section may be computationally feasible to apply.  However, the denominator of the conditional density for $\gamma_i$ above also includes an integral.  This "normalizing constant" is exactly the (marginal) likelihood function given earlier for the $i$th subject, where we are integrating out the random effect $\gamma_i$.  The joint density $f(\gamma_i, \boldsymbol{y}_o| \boldsymbol{\theta}^{(t)}) = f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{(t)})\phi(\gamma_i| 0, (\sigma^2_{\gamma})^{(t)})$ in the numerator is easily evaluable assuming $\gamma_i$ is known.  However, sampling from $f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})$ is not straight-forward, as no standard statistical software is available to directly sample from this distribution.  

For these reasons we can utilize importance sampling with standardized weights to approximate this integral, as discussed in the last lecture. This takes care of the normalizing constant during integration, avoiding having to evaluate it separately. For low-dimensions, we could use the Laplace approximation of AGQ to approximate this integral, but would not be scalable to higher dimesions. 

That is, we can approximate the Q-function with respect to subject $i$ such that

\begin{aligned}
Q_i(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) &= \int \left[\log f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) + \log\phi(\gamma_i| 0, \sigma^2_{\gamma})\right]f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})d\gamma_i\\
&= \int \left[\log f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) + \log\phi(\gamma_i| 0, \sigma^2_{\gamma})\right]\frac{f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})}{g(\gamma_i)}g(\gamma_i)d\gamma_i\\
&= \int \left[\sum_{j = 1}^{5}\log f(y_{ij} | \lambda_{ij}) + \log\phi(\gamma_i| 0, \sigma^2_{\gamma})\right]\frac{f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})}{g(\gamma_i)}g(\gamma_i)d\gamma_i\\
&=\frac{1}{M}\sum_{k = 1}^M\left[\sum_{j = 1}^{5}\log f(y_{ij} | \lambda_{ijk}) + \log \phi(X_k| 0, \sigma^2_{\gamma})\right]w(\boldsymbol{X}_k)
\end{aligned}

where $w(\boldsymbol{X}_k) = \frac{ w^*(\boldsymbol{X}_k)}{\sum_{k =  1}^M w^*(\boldsymbol{X}_k)}$, $w^*(\boldsymbol{X}_k) = \frac{\prod_{j = 1}^{5}f(y_{ij} | \lambda_{ijk}^{(t)})\phi(X_k| 0, (\sigma^2_{\gamma})^{(t)})}{g(\boldsymbol{X}_k)}$, $\log(\lambda_{ijk}) = \boldsymbol{x}_{ij}^T\boldsymbol{\beta} + X_k$, and $X_1,\ldots,\X_M$ are iid draws from some importance sampling distribution $g$. It should be noted that in the normalizing constant depends $\boldsymbol{\theta}^{(t)}$ and not $\boldsymbol{\theta}$, and therefore is irrelevant in the M-step. 

For the M-step, we can use again off-the shelf maximization routines for the above approximation for the Q-function.  

The question now is what is a suitable choice for $g$?  Similar to AGQ and the Laplace Approximation, we may choose to sample from a Student t importance density whose mean and variance match the mode and curvature of $f(\gamma_i \boldsymbol{y}_o| \boldsymbol{\theta}^{(t)})$. [Additional detail can be found here](https://www.jstor.org/stable/2680750?seq=1#metadata_info_tab_contents).  In general however, the best choice may not be clear. 

As the dimension increases, MCI and importance sampling retains its advantage over the quadrature-based approaches, as mentioned in the prior lecture.  However, alternative sampling approaches, such as some of the MCMC methods discussed in this lecture, may be more appropriate as we will see later. For importance sampling in particular, there may not always be a clear choice of $g$, which is critical to its success.  The choice between importance sampling and alternative approaches for evaluating the conditional expectation above may vary depending on the problem.  We will discuss these alternatives and how to choose between them in this lecture.  More generally, these alternative approaches provide a means to sample exactly or approximately from distributions where it is difficult to do so, such as from $f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})$.  

# Exact/Direct sampling (not MCMC)

In the above example we find that we do not have a clear means to generate samples from the conditional density of $\gamma_i$.  We know that  $f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})$ is known up to a proportionality constant, but we do not have a clear means to sample from this distribution to allow for MCI to be applied.  One way to do this is to use a category of methods called "direct" or "exact" sampling, where one can directly sample from the target distribution $f$ rather than avoiding it completely (importance sampling) or approximately sampling it (MCMC).  

We will show for the two classes of methods in this section, Rejection Sampling and Adaptive Rejection Sampling, that the samples drawn here are **independent** and originally drawn from an alternative distribution that is easier to sample from.  The latter sounds similar to importance sampling, with an important distinction: we are actually drawing samples from $f$, rather than drawing samples from $g$ and reweighting them in the integration process. These methods are applicable when $f$ is only known up to a proportionality constant.  Using the Rejection Sampling, we can generate sample from any density $f$ as long as we know the closed form of $f$.

## Rejection Sampling

If the target distribution $f(x)$ can be calculated, at least up to a proportionality constant, then we can use Rejection Sampling to directly obtain a random draw from the target distribution. The general idea behind Rejection Sampling is that we first sample from an alternative distribution $g$ to generate **candidate samples**.  We then correct for the fact that these samples came from another distribution through the random rejection of some of the samples, effectively correcting the original sampling probabilities.  Importance sampling in contrast directly weights samples from $g$ during the computation of the integral, rather than attempting to directly simulate samples from $f$ itself.  

Let us assume that we can calculate $g(x)$ easily and denote $e(\cdot)$ as the "envelope" function such that $$e(x) = \frac{g(x)}{\alpha} > f(x)$$ for all $x$ where $f(x) > 0$.  Here $\alpha \leq 1$ and is a constant.  That is, for all regions where $f(x)$ is nonzero, $g(x)$ is greater than or equal to $f(x)$ (hence the term "envelope").

Then, we implement Rejection Sampling using the following procedure: 

1.  Sample the candidate $Y ∼ g$
2.  Sample $U ∼ U(0, 1)$
3.  Reject the candidate sample $Y$ if $U > f(Y)/e(Y)$. Return to step 1.
4.  If $U < f(Y)/e(Y)$, keep Y. Set X = Y, and consider X to be an element
of the random sample from the target distribution . Return to step 1 until you have accumulated a
sample of the desired size.

The retained draws then represent an i.i.d. sample from target distribution $f$.  Here, $\alpha$ can be interpreted as the expected proportion of candidates that are accepted.  We often repeat the steps above until we obtain the desired number of samples.  The total number of candidate draws needed to obtain that size sample depends on how effcient the algorithm is in accepting samples (mediated by $\alpha$).  Clearly, this is highly dependent on the choice of $e$.  We will address how best to select $e$ later on in this section. 

### Why does this work?

A formal proof is given in GH 6.2.3, but we illustrate this graphically below.  The rejection rule in Step 3 above is equivalent to sampling $U\rvert y  \sim Unif(0, e(y))$ and keeping the value $y$ if $U < f(y)$ (see figure below from GH Fig 6.1) 

![Illustration of Rejection Sampling.](envelope.png){width=65%}

Lets suppose that our particular draw $y$ is at the vertical bar along the support of the target density (x-axis of fig). Then imagine sampling $U|Y = y$ uniformly along the vertical bar. In step 3, we reject this draw
with probability proportional bar length above $f(y)$ relative to the overall
length of the bar ($e(y)$). Therefore, one can view Rejection Sampling as sampling uniformly from the
two-dimensional region under the curve $e$ and then throwing away any draws falling
above $f$ and below $e$. Since sampling from $f$ is equivalent to sampling uniformly from the two-dimensional region under the curve labeled $f(x)$ and then ignoring the vertical coordinate, Rejection Sampling provides draws exactly from $f$.

In other words, this procedure is equivalent to first randomly selecting a point from the x-axis above by sampling from $g$, then drawing a sample $U$ from $U(0, e(y))$ along the y-axis.  If $f(y)<U<e(y)$ then we reject, otherwise we keep the sample.  In this manner, we are able to obtain an iid sample that is exactly from the target distribution $f$.

The shaded region in the figure above $f$ and below $e$ indicates the waste. Intuitively, 
a candidate draw is  likely to be rejected when $e(y) >> f (y)$, whereas envelopes
is only slightly bigger than $f$ everywhere produce fewer rejected draws, corresponding to $\alpha$ values near 1.

What if $f$ is only known up to a proportionality constant $c$ such as in our MCEM example above? For example, if we define $q(x) = f (x)/c$, where c is unknown, we can find an envelope $e$ such that $e(x) \geq q(x)$ for all $x$ for which $q(x) > 0$. A draw $Y = y$ is then rejected when $U > q(y)/e(y)$. This approach is similar to what we have described before, and we can show that that $c$ cancels out in the original proof in GH equation 6.2.3 when $f$ is replaced by $q$. That is, we can similary apply Rejection Sampling to this case.  The proportion of kept draws is $\alpha/c$.

Multivariate targets can also be sampled using Rejection Sampling, provided that
a suitable multivariate envelope can be constructed.

### Selecting an envelope function 
To produce an envelope we must know enough about the target function to effectively bound it.
This may require optimization, or a clever approximation to $f$ or $q$ in order to ensure
that the envelope $e$ can be chosen so that it exceeds $f$ everywhere. In general good Rejection Sampling envelopes have three properties: 

1.  They are easily constructed or confirmed to exceed the target everywhere
2.  They are easy to sample from 
3.  They generate few rejected draws.

Clearly this is not an exact science, and how best to do this will vary by problem.  This is one of the downsides of Rejection Sampling, where sometimes determining the best envelope function can be difficult or time consuming.  In some examples this step is simple, but that will not case in all situations. 

### Adaptive Rejection Sampling 
Adaptive Rejection Sampling is an automatic procedure to determine envelopes for the specific case where one has a 
a continuous, differentiable, log-concave target density that one would like to sample from.  Here, the envelope and
another function called the "squeezing function" are iteratively updated with the generation of samples. In this approach The amount of waste and the frequency with which $f$ must be evaluated both shrink as iterations increase. The implementation is more complicated than that of regular Rejection Sampling.  A tangent-based and a derivative-free based implementation of adaptive Rejection Sampling are available, the latter of which has been incorporated into the popular WinBUGS often used in Bayesian analyses.  We do not go into further detail here and direct students to GH 6.2.3.2 for more detail.  

### Example:  Sampling from a posterior distribution

Lets suppose we have 10 independent observations from GH Example 6.2 are observed from the model $X_i| \lambda \sim$  Poisson($\lambda$). We assume a lognormal prior distribution for $\lambda$, where  $\log(\lambda) \sim N(\log(4), 0.52)$. Here, we want to sample from the posterior distribution of $\lambda$ given the data.  

Lets start with a histogram of the data and calculating $\bar{x}$ below.

```{r}
x = c(8, 3, 4, 3, 1, 7, 2, 6, 2, 7)
hist(x)
print(mean(x))
```

Denote the likelihood as $L(\lambda|\boldsymbol{x})$ and the lognormal prior as $f(\lambda)$. We know that  $\hat{\lambda}  = \bar{x}$ maximizes the likelihood $L(\lambda|\boldsymbol{x})$ with respect to $\lambda$.   As a result, the unnormalized posterior, $q(\lambda|\boldsymbol{x}) = f(\lambda)L(\lambda|\boldsymbol{x})$
is bounded above by $e(\lambda) = f (\lambda)L(4.3|\boldsymbol{x})$. We can show this below:

```{r}
# unormalized posterior function
q = function(lambda, x){
  prod(dpois(x, lambda = lambda))*dlnorm(x = lambda,meanlog = log(4),sdlog = 0.5)
}

# envelope function
e = function(lambda, x){
  # fixing lambda at the mean of x
  prod(dpois(x, lambda = mean(x)))*dlnorm(x = lambda,meanlog = log(4),sdlog = 0.5)
}

# plot setup
lambda = seq(0, 20,,1000)
# envelope
plot(lambda, sapply(lambda, e, x = x),col = "black", type = 'l')
# q
lines(lambda, sapply(lambda, q, x = x), col = "red")
legend(c("e(x)","q(x)"), col = c(1, 2), lty = c(1,1) ,x = "topright")
```

One nice thing to note about the chosen envelope is that the prior is proportional to $e$. Therefore, $g$ is clearly is the lognormal prior (refer to definition of $g$ and $e$ fro the setup for Rejection Sampling).  So, we can start the Rejection Sampling algorithm by first sampling $\lambda_i$ from the lognormal prior and $U_i$ from a standard uniform distribution. Then $\lambda_i$ is kept if $U_i < \frac{q(\lambda_i|\boldsymbol{x})}{e(\lambda_i)} = \frac{L(\lambda_i|\boldsymbol{x})}{L(4.3|\boldsymbol{x})}$:

```{r}
set.seed(100)
# specify how many samples to keep
M = 100

# specify max number of iterations in case alpha is low (can increase later if needed)
maxit = 10^4

# specify vector to hold samples
posterior_lambda = rep(0, M)

# start rejection algorithm
i = index = 1
while(i <= M & index < maxit){
  
  # sample from lognormal prior
  lambdai = rlnorm(1, meanlog = log(4), sdlog = 0.5)
  
  # sample Ui from standard uniform
  Ui = runif(1)
  
  # calculate ratio
  Lq = prod(dpois(x, lambda = lambdai))
  Le = prod(dpois(x, lambda = mean(x)))
  r = Lq/Le
  
  # if Step 3 condition met, save sample
  if(Ui < r){
    posterior_lambda[i] = lambdai
    # update iterator
    i = i+1
  }
  
  # keep track of total number of samples drawn
  index = index +1
}

# acceptance rate
print(M/index)
```



Although not terribly efficient with an acceptance rate of about 26%, it is relatively easy to implement.

Lets make a histogram of the samples from the posterior:

```{r}
hist(posterior_lambda )
```

### Pros and cons of Rejection Sampling 

So, to summarize we have the following advantages and disadvantages of Rejection Sampling 

*  Pros
    *  Sample is truly iid from target density $f$, no approximation
*  Cons
    *  Designing a good envelope function $e$ may not be straightforward or easy to do
    *  Many candidate draws may be needed to obtain a sample of size $M$, depends on choice of $e$ 
    *  As the dimension of the integral groups, the above problem gets worse (acceptance rate is lower)
    
Regarding the last point above, it becomes harder in general to accept multivariate candidate draws in higher dimensions.  One way to think about this is to envision a multivariate version of the prior example where we  samples from the posterior of a function poisson-lognormal mixture.  The envelope function here would be defined similarly, except we would maximize over a multivariate space in the likelihood function rather than a univariate one, and fix the mean of the likelihood to this MLE.  Ideally, each candidate draw, say $\boldsymbol{X}_i$ from the multivariate prior $g$ in this case would be close to the MLE, implying that $f(\boldsymbol{X}_i)$ is close to $e(\boldsymbol{X}_i)$, leading to likely acceptance of this sample.  As we increase the dimension to say $d = 10$, the probability that a drawn vector will be close to the MLE in every dimension becomes harder and harder. 

In higher dimensional settings we will see that the MCMC approaches discussed in the second half of this lecture are more efficient relative to Rejection Sampling.  

###  Application to the MCEM Example

We can similarly apply Rejection Sampling to draw iid samples from the target density $f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})  = f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{(t)})\phi(\gamma_i| 0, (\sigma^2_{\gamma})^{(t)})$.  The question here is what is the best envelope function to use.

Similar to the previous example, one obvious option is simply $e(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) = f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{*(t)})\phi(\gamma_i| 0, \sigma^2_{\gamma})$, where $\boldsymbol{\lambda}_i^{*(t)} = (\lambda_{i1}^{*(t)},\ldots, \lambda_{i5}^{*(t)})$, $\log(\lambda_{ij}^{*(t)}) = \boldsymbol{x}_{ij}^T\boldsymbol{\beta}^{(t)} + \hat{\gamma}_i$ and $\hat{\gamma}_i$ is maximizer of $f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{(t)})$ with respect to $\gamma_i$. This can be done quite easily within the glm framework, as we will show below.  

Then similar to before, our proposal density $g$ can be $\phi(\gamma_i| 0, (\sigma^2_{\gamma})^{(t)})$.  We also draw $U$ separately from a standard uniform distribution.  Then, candidate $X$ drawn from $g$ is kept if $U < \frac{q(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})}{e(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})} = \frac{f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{(t)})}{f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}^{*(t)})}$, where $\phi(\gamma_i| 0, (\sigma^2_{\gamma})^{(t)})$ cancels out in the numerator and denominator.  

We will use this sampling step to perform MCI in the E-step.   In contrast to the importance sampling setup given earier, we can approximate the Q-function in the i$th$ sample using

\begin{aligned}
Q_i(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) &= \int \left[\log (f(\boldsymbol{y}_{i} | \boldsymbol{\lambda}_{i}) + \log\left(\phi(\gamma_i| 0, \sigma^2_{\gamma})\right)\right]f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})d\gamma_i\\
&= \int \left[\sum_{j = 1}^{5}\log(f(y_{ij} | \lambda_{ij}) + \log\left(\phi(\gamma_i| 0, \sigma^2_{\gamma})\right)\right]f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})d\gamma_i\\
&=\frac{1}{M}\sum_{k = 1}^M\left[\sum_{j = 1}^{5}\log\left(f(y_{ij} | \lambda_{ijk})\right) + \log\left(\phi(X_k| 0, \sigma^2_{\gamma})\right)\right]
\end{aligned}

since we are now able to draw iid samples from the posterior using Rejection Sampling.  Here, $\log(\lambda_{ijk}) = \boldsymbol{x}_{ij}^T\boldsymbol{\beta} + X_k$, and $X_1,\ldots,\X_M$ are iid draws from the target density $f(\gamma_i | \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})$. 

Then, we can carry out the M step using standard optimization routines.  For $\boldsymbol{\beta}$, an easier approach is simply using poisson regression, repeating each observation $\boldsymbol{y_{i}}$ and corresponding predictor in the design matrix $M$ times, filling in $\boldsymbol{X}_k$ for each repeated row for $\gamma_i$ in the linear predictor above.  Each repeated row is weighted with $1/M$.  We can also show that the solution for $\sigma^2_{\gamma}$ is simply the sum of the squared samples divided by $nM$.  

First, lets set up the rejection sampler:

```{r}
# function for Rejection Sampling, utilized in E-step of MCEM
rejection.sample.gamma.posterior = function(yi, M, maxit, betat, s2gammat, trace = 0 ){
  
  ## yi is a 5 x 1 vector containing the observations for sample i
  ## M is the total number of iid samples desired from the posterior
  ## maxit is the max number of iterations (to prevent runaway looping)
  ## betat is  the estimate for beta at EM iteration t
  ## s2gammat is the estimate for s2gamma at EM iteration t
  
  ## Returns list object with samples and acceptance rate
  
  ## envelope function setup
    # obtain gamma_hat for envelope function, optimizing over gammai
    # can use any optimization routine, here using IRLS holding XB fixed as an offset
    gamma_hat = glm(yi ~ 1, family = poisson(), offset = betat[1] + betat[2]*1:5)$coef
    
    # calculate lambda values for rejection ratio denominator
    lambda_star = exp(betat[1] + betat[2]*1:5 + gamma_hat)
    
    # calculate rejection ratio denominator
    Le = prod(dpois(yi, lambda = lambda_star))  
    
  ## specify vector to hold samples
  posterior_gammai = rep(0, M)
    
  ## start rejection sampler
  i = index = 1
  while(i <= M & index < maxit){
    # sample from normal prior
    gammai = rnorm(1, 0, sqrt(s2gammat))
    
    # sample Ui from standard uniform
    U = runif(1)
    
    # calculate lambda values for rejection ratio numerator
    lambdai = exp(betat[1] + betat[2]*1:5 + gammai)
    
    # calculate ratio numerator
    Lq = prod(dpois(yi, lambda = lambdai))

    # calculate ratio
    r = Lq/Le
    
    # if step 3 met, save sample
    if(U < r){
      posterior_gammai[i] = gammai
      # update iterator
      i = i+1
    }
    
    # keep track of total number of samples drawn
    index = index +1
  }
  
  # acceptance rate
  if(trace > 0) print(M/index)

  return(list(gammai = posterior_gammai, ar = M/index))
}
```

Now that we have that setup, lets now write out the setup for the EM algorithm in this example.

```{r}
## read in the data
alz = read.table("alzheimers.dat", header = T)

## initial parameters
  tol = 10^-5
  maxit = 100
  iter = 0
  eps = Inf
  qfunction = -10000 # using Qfunction for convergence
  
## starting values
  beta = c(1.804, 0.165)
  s2gamma =  0.000225 + .01 
  

## fix at 1000 draws from posterior per observation
  M = 1000

## For M-step use only: repeat the data from same observation M times 
  # create repeat index vector
  repeat_index = NULL
  for(i in 1:22) repeat_index = c(repeat_index, rep(which(alz$subject == i), M)) 
  head(repeat_index, 10)
  
  # now create repeated dataset
  alz_M_aug = alz[repeat_index,]
  
  # add another column to hold the M poterior draws per gamma_i
  rejection.samples = rep(0, nrow(alz_M_aug))
  alz_M_aug = cbind(alz_M_aug, rejection.samples) 
  
  # each individual's data repeated M times, last col to be filled with RS draws
  head(alz_M_aug, 10) 
  
  # create design matrix to hold fixed effects covariate matrix pertaining to alz_M_aug
  X_m_aug = model.matrix(~ 1 + month, data = alz_M_aug)
  
## Dimensions of original vs augmented matrix for M step
  dim(alz)     # originally (22*5) rows
  dim(alz_M_aug) # now (22*5*M) rows

```

Lets test out the Rejection sampler on just the first sample, given the starting values:

```{r}
# run RS
test_sample = rejection.sample.gamma.posterior(
                yi = alz$words[alz$subject == 1], 
                M = M, 
                maxit = maxit*M, 
                betat = beta, 
                s2gammat = s2gamma 
        )

# acceptance rate
print(test_sample$ar)

# mean
print(mean(test_sample$gammai))

# variance
print(var(test_sample$gammai))

#histogram
hist(test_sample$gammai)
```


Now that thats out of the way, lets apply EM.

```{r}
  
  start = Sys.time()
  while(eps > tol & iter < maxit){
  
  ## save old qfunction
    qfunction0 = qfunction
  
  ## Begin E-step

    posterior.rejection.samples = list()
    # looping over subjects
    for(i in 1:max(alz$subject)){
      # draw M samples from the posterior for gamma_i 
      posterior.rejection.samples[[i]] = 
        rejection.sample.gamma.posterior(
              yi = alz$words[alz$subject == i], 
              M = M, 
              maxit = maxit*M, 
              betat = beta, 
              s2gammat = s2gamma 
        )
      
      # fill in last col of list with drawn samples
      alz_M_aug$rejection.samples[alz_M_aug$subject == i] =
           rep(posterior.rejection.samples[[i]]$gammai, each = 5)
    
    } 
    ## End E-step
    
    ## Calculate qfunction
    lambda_M_aug = exp(X_m_aug %*% beta + alz_M_aug$rejection.samples)
    qfunction = dpois(alz_M_aug$words, lambda = lambda_M_aug, log = T) +
                   dnorm(alz_M_aug$rejection.samples, mean = 0, sd = sqrt(s2gamma), log = T)
    qfunction = sum(qfunction)/M # average over M
    
    ## Calculate relative change in qfunction
    eps  = abs(qfunction-qfunction0)/abs(qfunction0)
  
  ## update iterator
    iter = iter + 1
    if(iter == maxit) warning("Iteration limit reached without convergence")
    
  ## start M-step
    # s2gamma
    s2gamma = sum(alz_M_aug$rejection.samples[seq(1, nrow(alz_M_aug), by = 5)]^2)/(M*max(alz$subject))
    
    # beta, poisson glm with gammai's filled in with M posterior draws/subject
    # averaging over M draws, so weight = 1/M
    # posterior draws for gamma_i area are filled in offsets
    fit = glm(alz_M_aug$words ~ X_m_aug -1, 
              family = poisson(), 
              weights = rep(1/M, nrow(X_m_aug)), 
              offset = alz_M_aug$rejection.samples
            )
    beta = as.vector(fit$coefficients)
    
  ## print out info to keep track
    cat(sprintf("Iter: %d Qf: %.3f s2gamma: %f beta0: %.3f beta0:%.3f eps:%f\n",iter, qfunction,s2gamma, beta[1],beta[2], eps))
}

end = Sys.time()
print(end - start)
```

This is pretty close to what we get from glmer:

```{r}
library(lme4)
fit = glmer(words ~ month + (1|subject), family = poisson(), data = alz)
print(summary(fit))
```

In addition, the posterior modes of the random effects are also similar:

```{r}
# glmer 
print(ranef(fit)$subject)

# RS
print(mean(posterior.rejection.samples[[1]]$gammai))
```

We can also take a look at the final augmented matrix to see what it looks like 

```{r}
print(head(alz_M_aug, 10))
```

